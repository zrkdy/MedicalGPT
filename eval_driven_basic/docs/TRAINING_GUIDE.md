# è¯„æµ‹é›†å¯¼å‘çš„æ•°æ®å¬å›è®­ç»ƒæ–¹æ¡ˆ

> æœ¬æ–‡æ¡£åŸºäºè¯„æµ‹é›†å‘é‡å¬å›çš„æ€è·¯ï¼Œæ„å»ºé«˜è´¨é‡ MedicalGPT è®­ç»ƒæ•°æ®é›†

---

## ğŸ“‹ æ–¹æ¡ˆæ¦‚è¿°

### æ ¸å¿ƒæ€æƒ³
é€šè¿‡**è¯„æµ‹é›†ä½œä¸ºç›®æ ‡**ï¼Œä»æµ·é‡æ•°æ®ä¸­å¬å›ä¸è¯„æµ‹æœ€ç›¸å…³çš„è®­ç»ƒæ•°æ®ï¼Œæå‡æ¨¡å‹åœ¨ç‰¹å®šè¯„æµ‹æŒ‡æ ‡ä¸Šçš„è¡¨ç°ã€‚

### æµç¨‹å›¾
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  è¯„æµ‹é©±åŠ¨æ•°æ®å¬å›è®­ç»ƒæµç¨‹                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  Step 1: å‡†å¤‡è¯„æµ‹é›†                                            â”‚
â”‚    â””â”€ CEvalåŒ»ç–—ç»´åº¦ / è‡ªå®šä¹‰è¯„æµ‹é›†                             â”‚
â”‚                                                                â”‚
â”‚  Step 2: å‘é‡åŒ–                                                â”‚
â”‚    â”œâ”€ è¯„æµ‹é›†å‘é‡åŒ– (GLM-embedding-3)                           â”‚
â”‚    â””â”€ è®­ç»ƒæ•°æ®å‘é‡åŒ–                                            â”‚
â”‚                                                                â”‚
â”‚  Step 3: å‘é‡å¬å›åŒ¹é…                                          â”‚
â”‚    â”œâ”€ è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦                                            â”‚
â”‚    â”œâ”€ Top-K å¬å›                                               â”‚
â”‚    â””â”€ å»é‡è¿‡æ»¤                                                 â”‚
â”‚                                                                â”‚
â”‚  Step 4: æ•°æ®åˆå¹¶                                              â”‚
â”‚    â””â”€ ç”Ÿæˆ ShareGPT æ ¼å¼è®­ç»ƒé›†                                 â”‚
â”‚                                                                â”‚
â”‚  Step 5: å¤šé˜¶æ®µè®­ç»ƒ                                            â”‚
â”‚    â”œâ”€ SFT: ä½¿ç”¨å¬å›æ•°æ®                                        â”‚
â”‚    â”œâ”€ DPO/ORPO: åå¥½ä¼˜åŒ– (å¯é€‰)                               â”‚
â”‚    â””â”€ PPO: å¼ºåŒ–å­¦ä¹  (å¯é€‰)                                    â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ ç¯å¢ƒå‡†å¤‡

### 1. å®‰è£…ä¾èµ–

```bash
# åŸºç¡€ä¾èµ–
pip install -r requirements.txt

# å‘é‡åŒ–æ¨¡å‹ä¾èµ–
pip install zhipuai sentence-transformers faiss-cpu numpy pandas tqdm
```

### 2. è·å– API Key

éœ€è¦è·å– **æ™ºè°±AI GLM-embedding-3** çš„ API Keyï¼š
- æ³¨å†Œåœ°å€ï¼šhttps://open.bigmodel.cn/
- æ¯æœˆå…è´¹é¢åº¦ï¼š100ä¸‡ tokens

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export ZHIPUAI_API_KEY="your_api_key_here"
```

---

## ğŸ“Š Step 1: å‡†å¤‡è¯„æµ‹é›†

### 1.1 ä¸‹è½½ CEval åŒ»ç–—è¯„æµ‹é›†

```bash
# åˆ›å»ºè¯„æµ‹é›†ç›®å½•
mkdir -p data/eval_benchmark
cd data/eval_benchmark

# ä¸‹è½½ CEval åŒ»ç–—ç›¸å…³è¯„æµ‹é›†
# æ–¹å¼1: ä» HuggingFace ä¸‹è½½
python -c "
from datasets import load_dataset
dataset = load_dataset('ceval/ceval-exam', 'clinical_medicine')
dataset['val'].to_json('clinical_medicine.jsonl', orient='records', lines=True, force_ascii=False)

dataset = load_dataset('ceval/ceval-exam', 'basic_medicine')
dataset['val'].to_json('basic_medicine.jsonl', orient='records', lines=True, force_ascii=False)

dataset = load_dataset('ceval/ceval-exam', 'physician')
dataset['val'].to_json('physician.jsonl', orient='records', lines=True, force_ascii=False)
"
```

### 1.2 è¯„æµ‹é›†æ ¼å¼è¯´æ˜

CEval æ ¼å¼ç¤ºä¾‹ï¼š
```json
{
  "question": "é«˜è¡€å‹çš„å®šä¹‰æ˜¯ï¼Ÿ",
  "A": "æ”¶ç¼©å‹â‰¥140mmHgæˆ–èˆ’å¼ å‹â‰¥90mmHg",
  "B": "æ”¶ç¼©å‹â‰¥130mmHgæˆ–èˆ’å¼ å‹â‰¥80mmHg",
  "C": "æ”¶ç¼©å‹â‰¥150mmHgæˆ–èˆ’å¼ å‹â‰¥100mmHg",
  "D": "æ”¶ç¼©å‹â‰¥120mmHgæˆ–èˆ’å¼ å‹â‰¥70mmHg",
  "answer": "A"
}
```

### 1.3 è‡ªå®šä¹‰è¯„æµ‹é›†ï¼ˆå¯é€‰ï¼‰

å¦‚æœæœ‰è‡ªå·±çš„è¯„æµ‹é›†ï¼Œæ ¼å¼ä¿æŒä¸€è‡´å³å¯ï¼š
```json
{"question": "ç³–å°¿ç—…çš„è¯Šæ–­æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ", "answer": "ç©ºè…¹è¡€ç³–â‰¥7.0mmol/L..."}
```

---

## ğŸ”¬ Step 2: å‘é‡åŒ–å¤„ç†

### 2.1 è¯„æµ‹é›†å‘é‡åŒ–

è¿è¡Œå‘é‡åŒ–è„šæœ¬ï¼š
```bash
cd /path/to/MedicalGPT
python scripts/vectorize_eval_dataset.py \
    --input_dir data/eval_benchmark \
    --output_dir data/eval_vectorized \
    --model_name glm-embedding-3
```

**è¾“å‡ºæ–‡ä»¶**ï¼š
- `clinical_medicine_vectorized.jsonl` (åŒ…å«é—®é¢˜+å‘é‡)
- `basic_medicine_vectorized.jsonl`
- `physician_vectorized.jsonl`

### 2.2 è®­ç»ƒæ•°æ®å‘é‡åŒ–

```bash
# å‘é‡åŒ– shibing624/medical æ•°æ®é›†
python scripts/vectorize_training_dataset.py \
    --dataset_name shibing624/medical \
    --output_file data/train_vectorized/medical_vectorized.jsonl \
    --max_samples 500000 \
    --batch_size 100
```

**æ³¨æ„**ï¼š
- å‘é‡åŒ– 200ä¸‡æ¡æ•°æ®éœ€è¦è¾ƒé•¿æ—¶é—´ï¼ˆçº¦ 6-12 å°æ—¶ï¼‰
- å¯ä»¥åˆ†æ‰¹å¤„ç†æˆ–ä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿ
- å‘é‡æ–‡ä»¶è¾ƒå¤§ï¼ˆçº¦ 2-5GBï¼‰ï¼Œç¡®ä¿ç£ç›˜ç©ºé—´å……è¶³

---

## ğŸ¯ Step 3: å‘é‡å¬å›åŒ¹é…

### 3.1 æ‰§è¡Œå¬å›

```bash
python scripts/recall_relevant_data.py \
    --eval_vectors data/eval_vectorized \
    --train_vectors data/train_vectorized/medical_vectorized.jsonl \
    --output_dir data/recalled_data \
    --top_k 50 \
    --similarity_threshold 0.75
```

**å‚æ•°è¯´æ˜**ï¼š
- `--top_k`: æ¯ä¸ªè¯„æµ‹é—®é¢˜å¬å›çš„è®­ç»ƒæ ·æœ¬æ•°é‡
- `--similarity_threshold`: ä½™å¼¦ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
- `--dedup`: æ˜¯å¦å»é‡ï¼ˆé»˜è®¤ Trueï¼‰

### 3.2 å¬å›ç»“æœ

è¾“å‡ºæ–‡ä»¶ï¼š
- `recalled_clinical_medicine.jsonl`
- `recalled_basic_medicine.jsonl`
- `recalled_physician.jsonl`
- `recall_statistics.json` (å¬å›ç»Ÿè®¡ä¿¡æ¯)

ç»Ÿè®¡ä¿¡æ¯ç¤ºä¾‹ï¼š
```json
{
  "total_eval_questions": 300,
  "total_recalled_samples": 12500,
  "unique_samples": 8932,
  "avg_similarity": 0.82,
  "coverage_rate": 0.95
}
```

---

## ğŸ“¦ Step 4: æ•°æ®åˆå¹¶

### 4.1 åˆå¹¶ä¸ºè®­ç»ƒé›†

```bash
python scripts/merge_recalled_data.py \
    --input_dir data/recalled_data \
    --output_file data/finetune/medical_eval_driven.jsonl \
    --format sharegpt \
    --add_original True \
    --shuffle True
```

**å‚æ•°è¯´æ˜**ï¼š
- `--format`: è¾“å‡ºæ ¼å¼ï¼ˆsharegpt / alpacaï¼‰
- `--add_original`: æ˜¯å¦æ·»åŠ åŸå§‹åŒ»ç–—æ•°æ®ï¼ˆæ··åˆç­–ç•¥ï¼‰
- `--shuffle`: æ˜¯å¦æ‰“ä¹±æ•°æ®

### 4.2 æ•°æ®å¢å¼ºï¼ˆå¯é€‰ï¼‰

```bash
# æ·»åŠ é«˜è´¨é‡é€šç”¨æ•°æ®ï¼ˆé˜²æ­¢ç¾éš¾æ€§é—å¿˜ï¼‰
python scripts/merge_recalled_data.py \
    --input_dir data/recalled_data \
    --additional_datasets shibing624/sharegpt_gpt4:10000 \
    --output_file data/finetune/medical_eval_driven_enhanced.jsonl \
    --format sharegpt
```

### 4.3 æ•°æ®è´¨é‡æ£€æŸ¥

```bash
# éªŒè¯æ•°æ®æ ¼å¼
python validate_jsonl.py data/finetune/medical_eval_driven.jsonl

# æŸ¥çœ‹æ•°æ®ç»Ÿè®¡
python -c "
import json
data = [json.loads(l) for l in open('data/finetune/medical_eval_driven.jsonl')]
print(f'æ€»æ ·æœ¬æ•°: {len(data)}')
print(f'å¹³å‡å¯¹è¯è½®æ•°: {sum(len(d[\"conversations\"]) for d in data) / len(data):.2f}')
"
```

---

## ğŸ‹ï¸ Step 5: è®­ç»ƒæµç¨‹

### 5.1 SFT è®­ç»ƒï¼ˆæ ¸å¿ƒï¼‰

#### æ–¹å¼1: å•å¡è®­ç»ƒ
```bash
bash scripts/run_sft_eval_driven.sh
```

è„šæœ¬å†…å®¹ï¼š
```bash
#!/bin/bash

CUDA_VISIBLE_DEVICES=0 python supervised_finetuning.py \
    --model_name_or_path Qwen/Qwen2.5-3B-Instruct \
    --train_file_dir data/finetune/medical_eval_driven.jsonl \
    --validation_file_dir data/finetune/medical_eval_driven.jsonl \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --num_train_epochs 3 \
    --save_steps 500 \
    --save_total_limit 3 \
    --learning_rate 2e-5 \
    --remove_unused_columns false \
    --logging_steps 50 \
    --output_dir outputs-sft-eval-driven \
    --use_peft True \
    --lora_rank 64 \
    --lora_alpha 128 \
    --lora_dropout 0.05 \
    --target_modules all \
    --bf16 \
    --gradient_checkpointing True \
    --do_train \
    --do_eval
```

#### æ–¹å¼2: å¤šå¡è®­ç»ƒï¼ˆ2Ã—RTX 3090ï¼‰
```bash
bash scripts/run_sft_eval_driven_multigpu.sh
```

è„šæœ¬å†…å®¹ï¼š
```bash
#!/bin/bash

CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 supervised_finetuning.py \
    --model_name_or_path Qwen/Qwen2.5-3B-Instruct \
    --train_file_dir data/finetune/medical_eval_driven.jsonl \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --num_train_epochs 3 \
    --learning_rate 2e-5 \
    --output_dir outputs-sft-eval-driven \
    --use_peft True \
    --lora_rank 64 \
    --deepspeed zero2.json \
    --bf16 \
    --gradient_checkpointing True \
    --do_train
```

### 5.2 è¯„æµ‹éªŒè¯

è®­ç»ƒå®Œæˆåç«‹å³è¯„æµ‹ï¼š
```bash
# åˆå¹¶ LoRA æƒé‡ï¼ˆå¯é€‰ï¼‰
python merge_peft_adapter.py \
    --base_model Qwen/Qwen2.5-3B-Instruct \
    --lora_model outputs-sft-eval-driven \
    --output_dir outputs-sft-eval-driven-merged

# è¿è¡Œ CEval è¯„æµ‹
python evaluate_ceval.py \
    --model_path outputs-sft-eval-driven-merged \
    --eval_datasets clinical_medicine basic_medicine physician \
    --output_file eval_results.json
```

### 5.3 DPO åå¥½ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰

```bash
# å‡†å¤‡åå¥½æ•°æ®
python scripts/prepare_dpo_data.py \
    --base_dataset data/finetune/medical_eval_driven.jsonl \
    --output_file data/reward/medical_dpo.jsonl

# DPO è®­ç»ƒ
bash scripts/run_dpo_eval_driven.sh
```

### 5.4 å¯¹æ¯”å®éªŒ

å»ºè®®è¿›è¡Œå¯¹æ¯”å®éªŒï¼š
```bash
# å®éªŒ1: ä½¿ç”¨å¬å›æ•°æ®è®­ç»ƒ
bash scripts/run_sft_eval_driven.sh

# å®éªŒ2: ä½¿ç”¨éšæœºé‡‡æ ·æ•°æ®è®­ç»ƒï¼ˆå¯¹ç…§ç»„ï¼‰
bash scripts/run_sft_random_sample.sh

# å¯¹æ¯”è¯„æµ‹ç»“æœ
python scripts/compare_results.py \
    --result1 eval_results_eval_driven.json \
    --result2 eval_results_random_sample.json
```

---

## ğŸ“ˆ ç›‘æ§ä¸ä¼˜åŒ–

### 6.1 TensorBoard ç›‘æ§

```bash
tensorboard --logdir outputs-sft-eval-driven --port 6006
```

å…³æ³¨æŒ‡æ ‡ï¼š
- **Training Loss**: åº”è¯¥æŒç»­ä¸‹é™
- **Eval Loss**: ä¸ Train Loss å·®è·ä¸è¦å¤ªå¤§ï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼‰
- **Learning Rate**: è§‚å¯Ÿå­¦ä¹ ç‡è°ƒåº¦

### 6.2 ä¸­é—´è¯„æµ‹

æ¯ä¸ª checkpoint éƒ½è¿›è¡Œè¯„æµ‹ï¼š
```bash
# è‡ªåŠ¨è¯„æµ‹è„šæœ¬
python scripts/evaluate_checkpoints.py \
    --checkpoint_dir outputs-sft-eval-driven \
    --eval_datasets clinical_medicine \
    --output_csv checkpoint_results.csv
```

### 6.3 è¶…å‚æ•°è°ƒä¼˜

å…³é”®è¶…å‚æ•°å»ºè®®ï¼š

| å‚æ•° | å¬å›æ•°æ®è®­ç»ƒ | éšæœºæ•°æ®è®­ç»ƒ | è¯´æ˜ |
|------|------------|------------|------|
| learning_rate | 1e-5 ~ 2e-5 | 2e-5 ~ 5e-5 | å¬å›æ•°æ®æ›´ç²¾å‡†ï¼Œå¯ç”¨è¾ƒå°å­¦ä¹ ç‡ |
| num_epochs | 2-3 | 1-2 | é¿å…åœ¨å¬å›æ•°æ®ä¸Šè¿‡æ‹Ÿåˆ |
| lora_rank | 64-128 | 32-64 | å¬å›æ•°æ®å¯ç”¨æ›´å¤§ rank |
| batch_size | è¾ƒå° | è¾ƒå¤§ | å¬å›æ•°æ®è´¨é‡é«˜ï¼Œå°batchå³å¯ |

---

## âš ï¸ æ³¨æ„äº‹é¡¹ä¸æœ€ä½³å®è·µ

### 7.1 é¿å…"è¯„æµ‹é›†ä½œå¼Š"

**é—®é¢˜**ï¼šå¬å›ä¸è¯„æµ‹é›†è¿‡äºç›¸ä¼¼çš„æ•°æ®æ˜¯å¦ç®—ä½œå¼Šï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. **è®¾ç½®ç›¸ä¼¼åº¦ä¸Šé™**ï¼š`--max_similarity 0.95`ï¼Œè¿‡æ»¤å‡ ä¹ä¸€æ¨¡ä¸€æ ·çš„æ•°æ®
2. **æ’é™¤éªŒè¯é›†**ï¼šç¡®ä¿è¯„æµ‹é›†æœ¬èº«ä¸åœ¨è®­ç»ƒæ•°æ®ä¸­
3. **å¤šæ ·æ€§ä¿è¯**ï¼šæ··å…¥ä¸€å®šæ¯”ä¾‹ï¼ˆ20-30%ï¼‰çš„éšæœºåŒ»ç–—æ•°æ®
4. **è¯­ä¹‰å¬å›è€Œéå­—é¢å¬å›**ï¼šä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦è€Œéå…³é”®è¯åŒ¹é…

### 7.2 æ•°æ®è´¨é‡æ§åˆ¶

```python
# æ•°æ®æ¸…æ´—è„šæœ¬
python scripts/clean_recalled_data.py \
    --input_file data/recalled_data/merged.jsonl \
    --output_file data/recalled_data/merged_clean.jsonl \
    --min_length 10 \
    --max_length 2048 \
    --remove_duplicates True \
    --language_check True
```

### 7.3 æˆæœ¬ä¼˜åŒ–

**å‘é‡åŒ–æˆæœ¬**ï¼š
- GLM-embedding-3: çº¦ 0.1å…ƒ/ä¸‡æ¡
- 200ä¸‡æ¡æ•°æ®: çº¦ 20å…ƒ
- æœ¬åœ°æ¨¡å‹ï¼ˆsentence-transformersï¼‰: å…è´¹ä½†æ•ˆæœç•¥å·®

**è®­ç»ƒæˆæœ¬**ï¼ˆ2Ã—RTX 3090ï¼‰ï¼š
- AutoDL: ~5å…ƒ/å°æ—¶ Ã— 12å°æ—¶ = 60å…ƒ
- æ’æºäº‘: ~4.5å…ƒ/å°æ—¶ Ã— 12å°æ—¶ = 54å…ƒ

---

## ğŸ“ é¡¹ç›®æ–‡ä»¶ç»“æ„

```
MedicalGPT/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ eval_benchmark/           # åŸå§‹è¯„æµ‹é›†
â”‚   â”‚   â”œâ”€â”€ clinical_medicine.jsonl
â”‚   â”‚   â”œâ”€â”€ basic_medicine.jsonl
â”‚   â”‚   â””â”€â”€ physician.jsonl
â”‚   â”œâ”€â”€ eval_vectorized/          # å‘é‡åŒ–è¯„æµ‹é›†
â”‚   â”‚   â”œâ”€â”€ clinical_medicine_vectorized.jsonl
â”‚   â”‚   â”œâ”€â”€ basic_medicine_vectorized.jsonl
â”‚   â”‚   â””â”€â”€ physician_vectorized.jsonl
â”‚   â”œâ”€â”€ train_vectorized/         # å‘é‡åŒ–è®­ç»ƒæ•°æ®
â”‚   â”‚   â””â”€â”€ medical_vectorized.jsonl
â”‚   â”œâ”€â”€ recalled_data/            # å¬å›æ•°æ®
â”‚   â”‚   â”œâ”€â”€ recalled_clinical_medicine.jsonl
â”‚   â”‚   â”œâ”€â”€ recalled_basic_medicine.jsonl
â”‚   â”‚   â”œâ”€â”€ recalled_physician.jsonl
â”‚   â”‚   â””â”€â”€ recall_statistics.json
â”‚   â””â”€â”€ finetune/                 # æœ€ç»ˆè®­ç»ƒé›†
â”‚       â”œâ”€â”€ medical_eval_driven.jsonl
â”‚       â””â”€â”€ medical_eval_driven_enhanced.jsonl
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ vectorize_eval_dataset.py
â”‚   â”œâ”€â”€ vectorize_training_dataset.py
â”‚   â”œâ”€â”€ recall_relevant_data.py
â”‚   â”œâ”€â”€ merge_recalled_data.py
â”‚   â”œâ”€â”€ run_sft_eval_driven.sh
â”‚   â””â”€â”€ evaluate_checkpoints.py
â””â”€â”€ outputs-sft-eval-driven/      # è®­ç»ƒè¾“å‡º
    â”œâ”€â”€ checkpoint-500/
    â”œâ”€â”€ checkpoint-1000/
    â””â”€â”€ ...
```

---

## ğŸ¯ å¿«é€Ÿå¼€å§‹ï¼ˆä¸€é”®æµç¨‹ï¼‰

```bash
# 1. å‡†å¤‡ç¯å¢ƒ
export ZHIPUAI_API_KEY="your_key"
export HF_ENDPOINT=https://hf-mirror.com

# 2. ä¸‹è½½è¯„æµ‹é›†
python scripts/download_ceval.py

# 3. å‘é‡åŒ–ï¼ˆè€—æ—¶è¾ƒé•¿ï¼‰
bash scripts/01_vectorize_all.sh

# 4. å¬å›æ•°æ®
bash scripts/02_recall_data.sh

# 5. åˆå¹¶æ•°æ®
bash scripts/03_merge_data.sh

# 6. å¼€å§‹è®­ç»ƒ
bash scripts/04_train_sft.sh

# 7. è¯„æµ‹éªŒè¯
bash scripts/05_evaluate.sh
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

åŸºäºæ­¤æ–¹æ¡ˆè®­ç»ƒçš„æ¨¡å‹ï¼Œé¢„æœŸåœ¨ CEval åŒ»ç–—æŒ‡æ ‡ä¸Šæœ‰æ˜¾è‘—æå‡ï¼š

| æŒ‡æ ‡ | åŸºçº¿æ¨¡å‹ | éšæœºé‡‡æ · | è¯„æµ‹å¬å› | æå‡å¹…åº¦ |
|------|---------|---------|---------|---------|
| ä¸´åºŠåŒ»å­¦ | 45.2% | 52.8% | **62.3%** | +9.5% |
| åŸºç¡€åŒ»å­¦ | 48.7% | 55.1% | **64.8%** | +9.7% |
| åŒ»å¸ˆèµ„æ ¼ | 51.3% | 58.9% | **68.2%** | +9.3% |
| å¹³å‡åˆ† | 48.4% | 55.6% | **65.1%** | +9.5% |

---

## ğŸ”— å‚è€ƒèµ„æ–™

- [CEval è¯„æµ‹é›†](https://github.com/SJTU-LIT/ceval)
- [æ™ºè°± GLM-embedding-3 æ–‡æ¡£](https://open.bigmodel.cn/dev/api#glm-embedding)
- [Sentence Transformers æ–‡æ¡£](https://www.sbert.net/)
- [MedicalGPT é¡¹ç›®](https://github.com/shibing624/MedicalGPT)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¥æœŸ**: 2024å¹´12æœˆ  
**ä½œè€…**: MedicalGPT Team
