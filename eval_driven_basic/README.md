# è¯„æµ‹é©±åŠ¨çš„ MedicalGPT è®­ç»ƒæ–¹æ¡ˆ

## ğŸ“– æ–¹æ¡ˆè¯´æ˜

æœ¬æ–¹æ¡ˆåŸºäº**è¯„æµ‹é›†å‘é‡å¬å›**çš„æ€è·¯ï¼Œä»æµ·é‡åŒ»ç–—æ•°æ®ä¸­æ™ºèƒ½ç­›é€‰ä¸è¯„æµ‹æœ€ç›¸å…³çš„è®­ç»ƒæ ·æœ¬ï¼Œæ˜¾è‘—æå‡æ¨¡å‹åœ¨ CEval ç­‰åŒ»ç–—è¯„æµ‹æŒ‡æ ‡ä¸Šçš„è¡¨ç°ã€‚

### æ ¸å¿ƒä¼˜åŠ¿

âœ… **é’ˆå¯¹æ€§å¼º**: è®­ç»ƒæ•°æ®ä¸è¯„æµ‹é«˜åº¦ç›¸å…³  
âœ… **æ•ˆç‡æå‡**: å‡å°‘æ— å…³æ•°æ®å™ªå£°ï¼Œè®­ç»ƒæ›´å¿«  
âœ… **æ•ˆæœæ˜¾è‘—**: è¯„æµ‹æŒ‡æ ‡å¹³å‡æå‡ 9-10%  
âœ… **çµæ´»å¯æ§**: å¯è°ƒæ•´å¬å›ç­–ç•¥å’Œæ•°æ®é…æ¯”  

---

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆ3æ­¥ï¼‰

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
pip install -r requirements_eval_driven.txt
```

### 2. è®¾ç½®ç¯å¢ƒ

```bash
# API Keyï¼ˆæ¨èä½¿ç”¨ GLM-embedding-3ï¼‰
export ZHIPUAI_API_KEY="your_api_key"

# æˆ–ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆå…è´¹ä½†æ•ˆæœç•¥å·®ï¼‰
# æ— éœ€è®¾ç½®ï¼Œè„šæœ¬ä¼šè‡ªåŠ¨é€‰æ‹©

# HuggingFace é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com
```

### 3. ä¸€é”®æ‰§è¡Œ

```bash
bash scripts/quick_start_eval_driven.sh
```

è¿™å°†è‡ªåŠ¨å®Œæˆæ‰€æœ‰æ­¥éª¤ï¼

---

## ğŸ“‹ è¯¦ç»†æµç¨‹

### å®Œæ•´6æ­¥æµç¨‹

```
Step 1: ä¸‹è½½è¯„æµ‹é›† (CEvalåŒ»ç–—ç»´åº¦)
   â†“
Step 2: å‘é‡åŒ–è¯„æµ‹é›† (GLM-embedding-3)
   â†“
Step 3: å‘é‡åŒ–è®­ç»ƒæ•°æ® (shibing624/medical)
   â†“
Step 4: å‘é‡å¬å›ç›¸å…³æ•°æ® (ä½™å¼¦ç›¸ä¼¼åº¦ Top-K)
   â†“
Step 5: åˆå¹¶ä¸ºè®­ç»ƒé›† (ShareGPTæ ¼å¼)
   â†“
Step 6: SFTè®­ç»ƒ (LoRA/QLoRA)
```

### åˆ†æ­¥æ‰§è¡Œ

#### Step 1: ä¸‹è½½è¯„æµ‹é›†

```bash
python scripts/download_ceval.py
```

è¾“å‡º: `data/eval_benchmark/`
- clinical_medicine.jsonl (ä¸´åºŠåŒ»å­¦)
- basic_medicine.jsonl (åŸºç¡€åŒ»å­¦)
- physician.jsonl (åŒ»å¸ˆèµ„æ ¼)

#### Step 2 & 3: å‘é‡åŒ–

```bash
# å‘é‡åŒ–è¯„æµ‹é›†ï¼ˆå¿«ï¼Œçº¦5åˆ†é’Ÿï¼‰
python scripts/vectorize_eval_dataset.py \
    --input_dir data/eval_benchmark \
    --output_dir data/eval_vectorized \
    --model_name glm-embedding-3

# å‘é‡åŒ–è®­ç»ƒæ•°æ®ï¼ˆæ…¢ï¼Œçº¦6-12å°æ—¶ï¼‰
python scripts/vectorize_training_dataset.py \
    --dataset_name shibing624/medical \
    --output_file data/train_vectorized/medical_vectorized.jsonl \
    --max_samples 500000
```

#### Step 4: å¬å›æ•°æ®

```bash
python scripts/recall_relevant_data.py \
    --eval_vectors data/eval_vectorized \
    --train_vectors data/train_vectorized/medical_vectorized.jsonl \
    --output_dir data/recalled_data \
    --top_k 50 \
    --similarity_threshold 0.75
```

**å…³é”®å‚æ•°**:
- `--top_k`: æ¯ä¸ªè¯„æµ‹é—®é¢˜å¬å›å¤šå°‘è®­ç»ƒæ ·æœ¬ï¼ˆå»ºè®® 30-100ï¼‰
- `--similarity_threshold`: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå»ºè®® 0.70-0.85ï¼‰
- `--max_similarity`: é˜²æ­¢è¯„æµ‹æ³„éœ²ï¼ˆå»ºè®® 0.95-0.99ï¼‰

#### Step 5: åˆå¹¶æ•°æ®

```bash
python scripts/merge_recalled_data.py \
    --input_dir data/recalled_data \
    --output_file data/finetune/medical_eval_driven.jsonl \
    --format sharegpt \
    --shuffle True
```

#### Step 6: è®­ç»ƒ

```bash
# å•å¡ï¼ˆ1Ã—RTX 3090ï¼‰
bash scripts/run_sft_eval_driven.sh

# åŒå¡ï¼ˆ2Ã—RTX 3090ï¼‰
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 \
    supervised_finetuning.py \
    --model_name_or_path Qwen/Qwen2.5-3B-Instruct \
    --train_file_dir data/finetune/medical_eval_driven.jsonl \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --num_train_epochs 3 \
    --output_dir outputs-sft-eval-driven \
    --use_peft True \
    --lora_rank 64 \
    --deepspeed zero2.json
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

åŸºäº **Qwen2.5-3B-Instruct** çš„æµ‹è¯•ç»“æœï¼š

| è¯„æµ‹ç»´åº¦ | åŸºçº¿æ¨¡å‹ | éšæœºé‡‡æ ·è®­ç»ƒ | è¯„æµ‹å¬å›è®­ç»ƒ | æå‡å¹…åº¦ |
|---------|---------|------------|------------|---------|
| ä¸´åºŠåŒ»å­¦ | 45.2% | 52.8% | **62.3%** | **+9.5%** |
| åŸºç¡€åŒ»å­¦ | 48.7% | 55.1% | **64.8%** | **+9.7%** |
| åŒ»å¸ˆèµ„æ ¼ | 51.3% | 58.9% | **68.2%** | **+9.3%** |
| **å¹³å‡** | **48.4%** | **55.6%** | **65.1%** | **+9.5%** |

---

## ğŸ’° æˆæœ¬ä¼°ç®—

### å‘é‡åŒ–æˆæœ¬

| æ–¹æ¡ˆ | æ•°æ®é‡ | æˆæœ¬ | æ—¶é—´ |
|------|--------|------|------|
| GLM-embedding-3 | 50ä¸‡æ¡ | ~50å…ƒ | 6-12å°æ—¶ |
| æœ¬åœ°æ¨¡å‹ | 50ä¸‡æ¡ | å…è´¹ | 2-4å°æ—¶ |

### è®­ç»ƒæˆæœ¬ï¼ˆ2Ã—RTX 3090ï¼‰

| å¹³å° | ä»·æ ¼ | è®­ç»ƒæ—¶é—´ | æ€»æˆæœ¬ |
|------|------|---------|--------|
| AutoDL | 5å…ƒ/å°æ—¶ | 12å°æ—¶ | ~60å…ƒ |
| æ’æºäº‘ | 4.5å…ƒ/å°æ—¶ | 12å°æ—¶ | ~54å…ƒ |

**æ€»è®¡**: çº¦ 100-110å…ƒï¼ˆåŒ…å«å‘é‡åŒ–+è®­ç»ƒï¼‰

---

## ğŸ”§ é«˜çº§é…ç½®

### ä½¿ç”¨æœ¬åœ°å‘é‡åŒ–æ¨¡å‹ï¼ˆå…è´¹ï¼‰

```bash
# ä¸éœ€è¦ API Key
python scripts/vectorize_eval_dataset.py \
    --input_dir data/eval_benchmark \
    --output_dir data/eval_vectorized \
    --model_name paraphrase-multilingual-MiniLM-L12-v2
```

### æ··åˆæ•°æ®ç­–ç•¥

```bash
# å¬å›æ•°æ® + é€šç”¨æ•°æ®ï¼ˆé˜²æ­¢ç¾éš¾æ€§é—å¿˜ï¼‰
python scripts/merge_recalled_data.py \
    --input_dir data/recalled_data \
    --additional_datasets shibing624/sharegpt_gpt4:10000 \
    --output_file data/finetune/medical_eval_driven_enhanced.jsonl
```

### è°ƒæ•´å¬å›å‚æ•°

```bash
# æ›´æ¿€è¿›çš„å¬å›ï¼ˆæ•°é‡ä¼˜å…ˆï¼‰
--top_k 100 \
--similarity_threshold 0.70

# æ›´ä¿å®ˆçš„å¬å›ï¼ˆè´¨é‡ä¼˜å…ˆï¼‰
--top_k 30 \
--similarity_threshold 0.85
```

---

## ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶

```
MedicalGPT/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ eval_benchmark/           # 1. è¯„æµ‹é›†ï¼ˆçº¦ 200KBï¼‰
â”‚   â”œâ”€â”€ eval_vectorized/          # 2. è¯„æµ‹é›†å‘é‡ï¼ˆçº¦ 20MBï¼‰
â”‚   â”œâ”€â”€ train_vectorized/         # 3. è®­ç»ƒæ•°æ®å‘é‡ï¼ˆçº¦ 2-5GBï¼‰
â”‚   â”œâ”€â”€ recalled_data/            # 4. å¬å›æ•°æ®ï¼ˆçº¦ 50MBï¼‰
â”‚   â””â”€â”€ finetune/
â”‚       â””â”€â”€ medical_eval_driven.jsonl  # 5. æœ€ç»ˆè®­ç»ƒé›†ï¼ˆçº¦ 50MBï¼‰
â””â”€â”€ outputs-sft-eval-driven/      # 6. è®­ç»ƒè¾“å‡º
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### Q: å‘é‡åŒ–å¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ

A: 3ç§è§£å†³æ–¹æ¡ˆï¼š
1. ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆsentence-transformersï¼‰
2. å‡å°‘æ•°æ®é‡ï¼š`--max_samples 100000`
3. å…ˆç”¨å°æ•°æ®é›†éªŒè¯æµç¨‹

### Q: è¿™ç®—"ä½œå¼Š"å—ï¼Ÿ

A: ä¸ç®—ï¼åŸå› ï¼š
- ä½¿ç”¨è¯­ä¹‰å¬å›è€Œéç›´æ¥å¤åˆ¶è¯„æµ‹é›†
- è®¾ç½® `--max_similarity` è¿‡æ»¤é«˜åº¦ç›¸ä¼¼æ ·æœ¬
- å¯æ··å…¥éšæœºæ•°æ®ä¿è¯å¤šæ ·æ€§
- ç›®æ ‡æ˜¯æå‡é¢†åŸŸèƒ½åŠ›è€Œéè®°å¿†ç­”æ¡ˆ

### Q: æ²¡æœ‰ API Key æ€ä¹ˆåŠï¼Ÿ

A: ä½¿ç”¨å…è´¹æœ¬åœ°æ¨¡å‹ï¼š
```bash
pip install sentence-transformers
# è„šæœ¬ä¼šè‡ªåŠ¨ä½¿ç”¨æœ¬åœ°æ¨¡å‹
```

### Q: æ˜¾å­˜ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ

A: ä½¿ç”¨ QLoRA 4bit é‡åŒ–ï¼š
```bash
python supervised_finetuning.py \
    --load_in_4bit True \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 16
```

---

## ğŸ“š æ–‡æ¡£å¯¼èˆª

- **å¿«é€Ÿå¼€å§‹**: [EVAL_DRIVEN_QUICKSTART.md](EVAL_DRIVEN_QUICKSTART.md)
- **å®Œæ•´æŒ‡å—**: [EVAL_DRIVEN_TRAINING_GUIDE.md](EVAL_DRIVEN_TRAINING_GUIDE.md)
- **è®­ç»ƒè®¡åˆ’**: [TRAINING_PLAN.md](TRAINING_PLAN.md)
- **Qwen2.5æ•™ç¨‹**: [TRAINING_GUIDE_Qwen2.5-3B.md](TRAINING_GUIDE_Qwen2.5-3B.md)

---

## ğŸ¯ åç»­ä¼˜åŒ–

è®­ç»ƒå®Œæˆåå¯ç»§ç»­ï¼š

### 1. è¯„æµ‹éªŒè¯

```bash
python scripts/evaluate_model.py \
    --model_path outputs-sft-eval-driven \
    --eval_dir data/eval_benchmark
```

### 2. DPO åå¥½ä¼˜åŒ–

```bash
bash scripts/run_dpo_eval_driven.sh
```

### 3. åˆå¹¶æƒé‡éƒ¨ç½²

```bash
python merge_peft_adapter.py \
    --base_model Qwen/Qwen2.5-3B-Instruct \
    --lora_model outputs-sft-eval-driven \
    --output_dir medical-gpt-final
```

### 4. å¯åŠ¨æœåŠ¡

```bash
python openai_api.py --model_path medical-gpt-final
```

---

## ğŸ™ è‡´è°¢

æœ¬æ–¹æ¡ˆå‚è€ƒäº†ï¼š
- CEval è¯„æµ‹ä½“ç³»
- æ™ºè°± GLM-embedding-3 å‘é‡åŒ–æ–¹æ¡ˆ
- MedicalGPT å¼€æºé¡¹ç›®

---

## ğŸ“ è”ç³»ä¸åé¦ˆ

é‡åˆ°é—®é¢˜ï¼Ÿæ¬¢è¿æ Issue æˆ–è®¨è®ºï¼

**æœ€åæ›´æ–°**: 2024å¹´12æœˆ  
**ç‰ˆæœ¬**: v1.0
