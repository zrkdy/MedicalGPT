# MedicalGPT å®Œæ•´è®­ç»ƒæŒ‡å— - Qwen2.5-3B ç‰ˆæœ¬

## ç›®å½•
1. [æœåŠ¡å™¨é…ç½®è¦æ±‚](#1-æœåŠ¡å™¨é…ç½®è¦æ±‚)
2. [ç¯å¢ƒé…ç½®](#2-ç¯å¢ƒé…ç½®)
3. [æ•°æ®å‡†å¤‡](#3-æ•°æ®å‡†å¤‡)
4. [å®Œæ•´è®­ç»ƒæµç¨‹](#4-å®Œæ•´è®­ç»ƒæµç¨‹)
5. [æ¨¡å‹è¯„ä¼°ä¸éƒ¨ç½²](#5-æ¨¡å‹è¯„ä¼°ä¸éƒ¨ç½²)
6. [å¸¸è§é—®é¢˜ä¸ä¼˜åŒ–](#6-å¸¸è§é—®é¢˜ä¸ä¼˜åŒ–)

---

## 1. æœåŠ¡å™¨é…ç½®è¦æ±‚

### æ¨èé…ç½®ï¼ˆåŸºäº Qwen2.5-3Bï¼‰

| è®­ç»ƒé˜¶æ®µ | GPUè¦æ±‚ | æ˜¾å­˜è¦æ±‚ | å†…å­˜è¦æ±‚ | å­˜å‚¨è¦æ±‚ |
|---------|--------|---------|---------|---------|
| **é¢„è®­ç»ƒ (PT)** | 2Ã—A100/H100 (40GB) æˆ– 4Ã—RTX 4090 | â‰¥40GB | 64GB+ | 500GB+ |
| **ç›‘ç£å¾®è°ƒ (SFT)** | 2Ã—A100/H100 (40GB) æˆ– 2Ã—RTX 4090 | â‰¥24GB | 32GB+ | 200GB+ |
| **å¥–åŠ±å»ºæ¨¡ (RM)** | 1Ã—A100 (40GB) æˆ– 2Ã—RTX 4090 | â‰¥24GB | 32GB+ | 100GB+ |
| **DPOè®­ç»ƒ** | 2Ã—A100 (40GB) æˆ– 2Ã—RTX 4090 | â‰¥24GB | 32GB+ | 200GB+ |
| **PPOè®­ç»ƒ** | 2Ã—A100 (80GB) | â‰¥80GB | 64GB+ | 200GB+ |

### äº‘æœåŠ¡å™¨ç§Ÿèµå»ºè®®

**æ¨èå¹³å°ï¼š**
- **AutoDL**: æ€§ä»·æ¯”é«˜ï¼ŒæŒ‰å°æ—¶è®¡è´¹ï¼Œé€‚åˆå®éªŒ
- **æ’æºäº‘**: ç¨³å®šæ€§å¥½ï¼Œæ”¯æŒé•¿æ—¶é—´è®­ç»ƒ
- **é˜¿é‡Œäº‘/è…¾è®¯äº‘**: ä¼ä¸šçº§ç¨³å®šæ€§ï¼Œä»·æ ¼è¾ƒé«˜
- **AWS/Azure**: å›½é™…å¹³å°ï¼Œèµ„æºä¸°å¯Œ

**æ¨èé…ç½®ï¼š**
```
æ–¹æ¡ˆ1ï¼ˆç»æµå‹ï¼‰: 2Ã—RTX 4090 (24GB)
- ä»·æ ¼: ~8-12å…ƒ/å°æ—¶
- é€‚åˆ: SFT, RM, DPOè®­ç»ƒ
- è®­ç»ƒæ—¶é—´: é¢„è®¡ 24-48å°æ—¶å®ŒæˆSFT

æ–¹æ¡ˆ2ï¼ˆæ¨èå‹ï¼‰: 2Ã—A100 (40GB)
- ä»·æ ¼: ~15-20å…ƒ/å°æ—¶
- é€‚åˆ: æ‰€æœ‰è®­ç»ƒé˜¶æ®µ
- è®­ç»ƒæ—¶é—´: é¢„è®¡ 12-24å°æ—¶å®ŒæˆSFT

æ–¹æ¡ˆ3ï¼ˆé«˜é…å‹ï¼‰: 2Ã—A100 (80GB)
- ä»·æ ¼: ~25-35å…ƒ/å°æ—¶
- é€‚åˆ: PPOè®­ç»ƒ + å¤§è§„æ¨¡æ•°æ®
- è®­ç»ƒæ—¶é—´: æœ€å¿«ï¼Œæ”¯æŒæ›´å¤§batch size
```

---

## 2. ç¯å¢ƒé…ç½®

### 2.1 å…‹éš†é¡¹ç›®

```bash
# è¿æ¥åˆ°æœåŠ¡å™¨å
cd /root
git clone https://github.com/shibing624/MedicalGPT.git
cd MedicalGPT
```

### 2.2 åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
# ä½¿ç”¨ Condaï¼ˆæ¨èï¼‰
conda create -n medical python=3.10 -y
conda activate medical

# æˆ–ä½¿ç”¨ venv
python3 -m venv venv_medical
source venv_medical/bin/activate
```

### 2.3 å®‰è£…ä¾èµ–

```bash
# å®‰è£… PyTorch (CUDA 11.8)
pip install torch==2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# å®‰è£…é¡¹ç›®ä¾èµ–
pip install -r requirements.txt

# å®‰è£…é¢å¤–ä¾èµ–ï¼ˆQLoRAæ”¯æŒï¼‰
pip install bitsandbytes
pip install flash-attn --no-build-isolation

# éªŒè¯å®‰è£…
python -c "import torch; print(torch.cuda.is_available())"
python -c "import transformers; print(transformers.__version__)"
```

### 2.4 é…ç½® Hugging Face

```bash
# è®¾ç½® HF é•œåƒï¼ˆå›½å†…æœåŠ¡å™¨ï¼‰
export HF_ENDPOINT=https://hf-mirror.com

# ç™»å½• Hugging Faceï¼ˆç”¨äºä¸‹è½½ Qwen2.5-3Bï¼‰
pip install huggingface_hub
huggingface-cli login
# è¾“å…¥ä½ çš„ HF Token: hf_xxxxxxxxxxxxx
```

### 2.5 ç›®å½•ç»“æ„

```bash
MedicalGPT/
â”œâ”€â”€ data/                  # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ pretrain/         # é¢„è®­ç»ƒæ•°æ®ï¼ˆåŒ»ç–—æ–‡æœ¬ï¼‰
â”‚   â”œâ”€â”€ finetune/         # SFTæ•°æ®ï¼ˆåŒ»ç–—é—®ç­”å¯¹ï¼‰
â”‚   â””â”€â”€ reward/           # RM/DPOæ•°æ®ï¼ˆåå¥½æ•°æ®ï¼‰
â”œâ”€â”€ outputs-pt/           # é¢„è®­ç»ƒè¾“å‡º
â”œâ”€â”€ outputs-sft/          # SFTè¾“å‡º
â”œâ”€â”€ outputs-rm/           # RMè¾“å‡º
â”œâ”€â”€ outputs-dpo/          # DPOè¾“å‡º
â”œâ”€â”€ cache/                # æ¨¡å‹ç¼“å­˜
â””â”€â”€ logs/                 # è®­ç»ƒæ—¥å¿—
```

---

## 3. æ•°æ®å‡†å¤‡

### 3.1 é¢„è®­ç»ƒæ•°æ® (PT)

**æ ¼å¼è¦æ±‚ï¼š** çº¯æ–‡æœ¬ï¼Œæ¯è¡Œä¸€ä¸ªåŒ»ç–—æ–‡æ¡£

```bash
# åˆ›å»ºæ•°æ®ç›®å½•
mkdir -p data/pretrain

# ç¤ºä¾‹æ•°æ®æ ¼å¼ï¼ˆdata/pretrain/medical_corpus.txtï¼‰
```

**medical_corpus.txt ç¤ºä¾‹ï¼š**
```text
é«˜è¡€å‹æ˜¯ä¸€ç§å¸¸è§çš„å¿ƒè¡€ç®¡ç–¾ç—…ï¼Œä¸»è¦è¡¨ç°ä¸ºè¡€å‹æŒç»­å‡é«˜ã€‚æ²»ç–—æ–¹æ³•åŒ…æ‹¬ç”Ÿæ´»æ–¹å¼å¹²é¢„å’Œè¯ç‰©æ²»ç–—ã€‚å¸¸ç”¨é™å‹è¯æœ‰ACEIç±»ã€ARBç±»ã€åˆ©å°¿å‰‚ç­‰ã€‚
ç³–å°¿ç—…åˆ†ä¸º1å‹å’Œ2å‹ï¼Œ2å‹ç³–å°¿ç—…å 90%ä»¥ä¸Šã€‚ä¸»è¦æ²»ç–—è¯ç‰©åŒ…æ‹¬äºŒç”²åŒèƒã€ç£ºè„²ç±»ã€GLP-1å—ä½“æ¿€åŠ¨å‰‚ç­‰ã€‚æ‚£è€…éœ€è¦å®šæœŸç›‘æµ‹è¡€ç³–ã€‚
æ„Ÿå†’æ˜¯ç”±ç—…æ¯’å¼•èµ·çš„ä¸Šå‘¼å¸é“æ„ŸæŸ“ï¼Œä¸»è¦ç—‡çŠ¶åŒ…æ‹¬é¼»å¡ã€æµæ¶•ã€å’³å—½ã€å‘çƒ­ç­‰ã€‚æ²»ç–—ä»¥å¯¹ç—‡ä¸ºä¸»ï¼Œæ³¨æ„ä¼‘æ¯ï¼Œå¤šé¥®æ°´ã€‚
```

**è·å–åŒ»ç–—æ•°æ®ï¼š**
```bash
# 1. ä½¿ç”¨é¡¹ç›®è‡ªå¸¦çš„ç¤ºä¾‹æ•°æ®
# å·²åŒ…å«åœ¨ data/pretrain ä¸­

# 2. ä¸‹è½½å¼€æºåŒ»ç–—æ•°æ®é›†
# Chinese Medical Dialogue Dataset
wget https://huggingface.co/datasets/Toyhom/Chinese-medical-dialogue-data/resolve/main/train.json

# 3. å‡†å¤‡è‡ªå·±çš„åŒ»ç–—æ•°æ®
# æ ¼å¼è½¬æ¢è„šæœ¬ï¼ˆå°†jsonlè½¬ä¸ºtxtï¼‰
python scripts/convert_to_pretrain_format.py \
    --input_file your_medical_data.jsonl \
    --output_file data/pretrain/medical_corpus.txt
```

### 3.2 ç›‘ç£å¾®è°ƒæ•°æ® (SFT)

**æ ¼å¼è¦æ±‚ï¼š** JSONLæ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªå¯¹è¯

```json
{"conversations":[{"from":"human","value":"æ‚£è€…ï¼šåŒ»ç”Ÿï¼Œæˆ‘æœ€è¿‘æ€»æ˜¯å¤´æ™•ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿ"},{"from":"gpt","value":"åŒ»ç”Ÿï¼šå¤´æ™•çš„åŸå› å¾ˆå¤šï¼ŒåŒ…æ‹¬ä½è¡€å‹ã€é¢ˆæ¤ç—…ã€è´«è¡€ç­‰ã€‚å»ºè®®æ‚¨å…ˆæµ‹é‡è¡€å‹ï¼Œå¦‚æœè¡€å‹æ­£å¸¸ï¼Œå¯ä»¥åšä¸ªé¢ˆæ¤Xå…‰æ£€æŸ¥ã€‚å¹³æ—¶æ³¨æ„ä¼‘æ¯ï¼Œé¿å…çªç„¶èµ·ç«‹ã€‚"}]}
{"conversations":[{"from":"human","value":"æ„Ÿå†’äº†åƒä»€ä¹ˆè¯ï¼Ÿ"},{"from":"gpt","value":"æ„Ÿå†’ä¸€èˆ¬æ˜¯ç—…æ¯’æ„ŸæŸ“ï¼Œå¤šæ•°å¯ä»¥è‡ªæ„ˆã€‚å»ºè®®å¤šä¼‘æ¯ã€å¤šå–æ°´ï¼Œå¦‚æœç—‡çŠ¶ä¸¥é‡å¯ä»¥æœç”¨å¯¹ä¹™é…°æ°¨åŸºé…šé€€çƒ­ï¼Œé…åˆç»´ç”Ÿç´ Cã€‚å¦‚æœ3å¤©åç—‡çŠ¶æœªç¼“è§£ï¼Œè¯·åŠæ—¶å°±åŒ»ã€‚"}]}
```

**æ•°æ®å‡†å¤‡ï¼š**
```bash
mkdir -p data/finetune

# ä½¿ç”¨é¡¹ç›®è‡ªå¸¦æ•°æ®
cp data/finetune/medical_sft_1K_format.jsonl data/finetune/train.jsonl

# æˆ–ä¸‹è½½æ›´å¤šæ•°æ®
# ChatDoctoræ•°æ®é›†
wget https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k/resolve/main/data/train-00000-of-00001.parquet

# HuatuoGPTæ•°æ®é›†
git clone https://huggingface.co/datasets/FreedomIntelligence/HuatuoGPT-sft-data-v1
```

### 3.3 å¥–åŠ±å»ºæ¨¡/DPOæ•°æ® (RM/DPO)

**æ ¼å¼è¦æ±‚ï¼š** æ¯æ¡åŒ…å« chosen å’Œ rejected ä¸¤ä¸ªå›ç­”

```json
{"system":"","history":[],"question":"æ„Ÿå†’äº†æ€ä¹ˆåŠï¼Ÿ","response_chosen":"å»ºè®®æ‚¨å¤šä¼‘æ¯ã€å¤šå–æ°´ï¼Œä¿æŒå®¤å†…é€šé£ã€‚å¦‚æœå‡ºç°å‘çƒ­ç—‡çŠ¶å¯ä»¥æœç”¨å¯¹ä¹™é…°æ°¨åŸºé…šé€€çƒ­ã€‚å¦‚æœç—‡çŠ¶æŒç»­3å¤©ä»¥ä¸Šæˆ–å‡ºç°å‘¼å¸å›°éš¾ï¼Œè¯·åŠæ—¶å°±åŒ»ã€‚","response_rejected":"å–çƒ­æ°´å°±è¡Œäº†ã€‚"}
{"system":"ä½ æ˜¯ä¸€åä¸“ä¸šåŒ»ç”Ÿ","history":[],"question":"é«˜è¡€å‹éœ€è¦ä¸€ç›´åƒè¯å—ï¼Ÿ","response_chosen":"é«˜è¡€å‹æ˜¯ä¸€ç§æ…¢æ€§ç–¾ç—…ï¼Œå¤šæ•°æ‚£è€…éœ€è¦é•¿æœŸæœè¯æ§åˆ¶ã€‚çªç„¶åœè¯å¯èƒ½å¯¼è‡´è¡€å‹åå¼¹ï¼Œå¢åŠ å¿ƒè„‘è¡€ç®¡äº‹ä»¶é£é™©ã€‚å»ºè®®åœ¨åŒ»ç”ŸæŒ‡å¯¼ä¸‹è°ƒæ•´ç”¨è¯ï¼Œå®šæœŸç›‘æµ‹è¡€å‹ã€‚","response_rejected":"è¡€å‹æ­£å¸¸äº†å°±å¯ä»¥åœè¯ã€‚"}
```

**æ•°æ®å‡†å¤‡ï¼š**
```bash
mkdir -p data/reward

# ä½¿ç”¨é¡¹ç›®è‡ªå¸¦æ•°æ®
cp data/reward/dpo_zh_500.jsonl data/reward/train.jsonl

# æˆ–ä»SFTæ•°æ®ç”Ÿæˆåå¥½æ•°æ®
python scripts/generate_preference_data.py \
    --sft_data data/finetune/train.jsonl \
    --output_file data/reward/preference_data.jsonl \
    --num_samples 5000
```

---

## 4. å®Œæ•´è®­ç»ƒæµç¨‹

### 4.1 é˜¶æ®µä¸€ï¼šå¢é‡é¢„è®­ç»ƒ (PT) - å¯é€‰

**ç›®çš„ï¼š** è®©æ¨¡å‹å­¦ä¹ åŒ»ç–—é¢†åŸŸçš„è¯­è¨€ç‰¹å¾å’Œä¸“ä¸šçŸ¥è¯†

**é¢„è®¡æ—¶é—´ï¼š** 12-24å°æ—¶ï¼ˆ10ä¸‡æ¡æ•°æ®ï¼Œ2Ã—A100ï¼‰

**åˆ›å»ºè®­ç»ƒè„šæœ¬ï¼š** `scripts/run_pt_qwen2.5-3b.sh`

```bash
#!/bin/bash
# å¢é‡é¢„è®­ç»ƒ - Qwen2.5-3B

CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2 pretraining.py \
    --model_name_or_path Qwen/Qwen2.5-3B \
    --train_file_dir ./data/pretrain \
    --validation_file_dir ./data/pretrain \
    --per_device_train_batch_size 2 \
    --per_device_eval_batch_size 2 \
    --do_train \
    --do_eval \
    --use_peft True \
    --seed 42 \
    --max_train_samples 100000 \
    --max_eval_samples 500 \
    --num_train_epochs 1 \
    --learning_rate 2e-4 \
    --warmup_ratio 0.05 \
    --weight_decay 0.01 \
    --logging_strategy steps \
    --logging_steps 10 \
    --eval_steps 500 \
    --eval_strategy steps \
    --save_steps 1000 \
    --save_strategy steps \
    --save_total_limit 3 \
    --gradient_accumulation_steps 16 \
    --preprocessing_num_workers 8 \
    --block_size 1024 \
    --group_by_length True \
    --output_dir outputs-pt-qwen2.5-3b \
    --overwrite_output_dir \
    --ddp_timeout 30000 \
    --logging_first_step True \
    --target_modules all \
    --lora_rank 16 \
    --lora_alpha 32 \
    --lora_dropout 0.05 \
    --torch_dtype bfloat16 \
    --bf16 \
    --device_map auto \
    --report_to tensorboard \
    --ddp_find_unused_parameters False \
    --gradient_checkpointing True \
    --cache_dir ./cache
```

**å¯åŠ¨è®­ç»ƒï¼š**
```bash
chmod +x scripts/run_pt_qwen2.5-3b.sh
nohup bash scripts/run_pt_qwen2.5-3b.sh > logs/pt_training.log 2>&1 &

# ç›‘æ§è®­ç»ƒ
tail -f logs/pt_training.log

# æŸ¥çœ‹TensorBoard
tensorboard --logdir outputs-pt-qwen2.5-3b --port 6006 --bind_all
```

**è®­ç»ƒå®Œæˆåï¼š**
- æ¨¡å‹æƒé‡ä¿å­˜åœ¨ï¼š`outputs-pt-qwen2.5-3b/checkpoint-xxxx/`
- LoRAæƒé‡ï¼š`adapter_model.bin` + `adapter_config.json`

### 4.2 é˜¶æ®µäºŒï¼šç›‘ç£å¾®è°ƒ (SFT) - å¿…éœ€

**ç›®çš„ï¼š** è®­ç»ƒæ¨¡å‹è¿›è¡ŒåŒ»ç–—é—®ç­”å¯¹è¯

**é¢„è®¡æ—¶é—´ï¼š** 4-8å°æ—¶ï¼ˆ1ä¸‡æ¡æ•°æ®ï¼Œ2Ã—A100ï¼‰

**åˆ›å»ºè®­ç»ƒè„šæœ¬ï¼š** `scripts/run_sft_qwen2.5-3b.sh`

```bash
#!/bin/bash
# ç›‘ç£å¾®è°ƒ - Qwen2.5-3B

# å¦‚æœè¿›è¡Œäº†PTï¼Œä½¿ç”¨PTåçš„æ¨¡å‹ï¼›å¦åˆ™ä½¿ç”¨åŸå§‹æ¨¡å‹
MODEL_PATH="Qwen/Qwen2.5-3B-Instruct"
# å¦‚æœå®Œæˆäº†PTï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
# MODEL_PATH="outputs-pt-qwen2.5-3b/checkpoint-best"

CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2 supervised_finetuning.py \
    --model_name_or_path $MODEL_PATH \
    --train_file_dir ./data/finetune \
    --validation_file_dir ./data/finetune \
    --per_device_train_batch_size 2 \
    --per_device_eval_batch_size 2 \
    --do_train \
    --do_eval \
    --template_name qwen \
    --use_peft True \
    --max_train_samples 10000 \
    --max_eval_samples 100 \
    --model_max_length 2048 \
    --num_train_epochs 3 \
    --learning_rate 2e-5 \
    --warmup_ratio 0.05 \
    --weight_decay 0.05 \
    --logging_strategy steps \
    --logging_steps 10 \
    --eval_steps 100 \
    --eval_strategy steps \
    --save_steps 500 \
    --save_strategy steps \
    --save_total_limit 3 \
    --gradient_accumulation_steps 16 \
    --preprocessing_num_workers 8 \
    --output_dir outputs-sft-qwen2.5-3b \
    --overwrite_output_dir \
    --ddp_timeout 30000 \
    --logging_first_step True \
    --target_modules all \
    --lora_rank 16 \
    --lora_alpha 32 \
    --lora_dropout 0.05 \
    --torch_dtype bfloat16 \
    --bf16 \
    --device_map auto \
    --report_to tensorboard \
    --ddp_find_unused_parameters False \
    --gradient_checkpointing True \
    --cache_dir ./cache
```

**å¯åŠ¨è®­ç»ƒï¼š**
```bash
chmod +x scripts/run_sft_qwen2.5-3b.sh
nohup bash scripts/run_sft_qwen2.5-3b.sh > logs/sft_training.log 2>&1 &

tail -f logs/sft_training.log
```

**åˆå¹¶LoRAæƒé‡ï¼ˆå¯é€‰ï¼Œç”¨äºæ¨ç†ï¼‰ï¼š**
```bash
python scripts/merge_lora.py \
    --base_model Qwen/Qwen2.5-3B-Instruct \
    --lora_model outputs-sft-qwen2.5-3b/checkpoint-best \
    --output_dir outputs-sft-qwen2.5-3b-merged
```

### 4.3 é˜¶æ®µä¸‰Aï¼šå¥–åŠ±å»ºæ¨¡ (RM) - ç”¨äºPPO

**ç›®çš„ï¼š** è®­ç»ƒä¸€ä¸ªæ‰“åˆ†æ¨¡å‹ï¼Œè¯„ä¼°å›ç­”è´¨é‡

**é¢„è®¡æ—¶é—´ï¼š** 2-4å°æ—¶ï¼ˆ5000æ¡æ•°æ®ï¼Œ1Ã—A100ï¼‰

**åˆ›å»ºè®­ç»ƒè„šæœ¬ï¼š** `scripts/run_rm_qwen2.5-3b.sh`

```bash
#!/bin/bash
# å¥–åŠ±å»ºæ¨¡ - Qwen2.5-3B

# åŸºäºSFTæ¨¡å‹è®­ç»ƒRM
SFT_MODEL="outputs-sft-qwen2.5-3b/checkpoint-best"

CUDA_VISIBLE_DEVICES=0 python reward_modeling.py \
    --model_name_or_path $SFT_MODEL \
    --train_file_dir ./data/reward \
    --validation_file_dir ./data/reward \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --per_device_eval_batch_size 2 \
    --do_train \
    --do_eval \
    --use_peft True \
    --seed 42 \
    --max_train_samples 5000 \
    --max_eval_samples 100 \
    --num_train_epochs 2 \
    --learning_rate 2e-5 \
    --warmup_ratio 0.05 \
    --weight_decay 0.001 \
    --logging_strategy steps \
    --logging_steps 10 \
    --eval_steps 100 \
    --eval_strategy steps \
    --save_steps 500 \
    --save_strategy steps \
    --save_total_limit 2 \
    --max_source_length 1024 \
    --max_target_length 512 \
    --output_dir outputs-rm-qwen2.5-3b \
    --overwrite_output_dir \
    --ddp_timeout 30000 \
    --logging_first_step True \
    --target_modules all \
    --lora_rank 16 \
    --lora_alpha 32 \
    --lora_dropout 0.05 \
    --bf16 \
    --torch_dtype bfloat16 \
    --device_map auto \
    --report_to tensorboard \
    --ddp_find_unused_parameters False \
    --remove_unused_columns False \
    --gradient_checkpointing True \
    --cache_dir ./cache
```

**å¯åŠ¨è®­ç»ƒï¼š**
```bash
chmod +x scripts/run_rm_qwen2.5-3b.sh
nohup bash scripts/run_rm_qwen2.5-3b.sh > logs/rm_training.log 2>&1 &
```

### 4.3 é˜¶æ®µä¸‰Bï¼šDPOè®­ç»ƒ (æ¨è) - æ›¿ä»£RM+PPO

**ç›®çš„ï¼š** ç›´æ¥ä»åå¥½æ•°æ®ä¼˜åŒ–æ¨¡å‹ï¼Œæ— éœ€å•ç‹¬çš„RM

**é¢„è®¡æ—¶é—´ï¼š** 4-6å°æ—¶ï¼ˆ5000æ¡æ•°æ®ï¼Œ2Ã—A100ï¼‰

**åˆ›å»ºè®­ç»ƒè„šæœ¬ï¼š** `scripts/run_dpo_qwen2.5-3b.sh`

```bash
#!/bin/bash
# DPOè®­ç»ƒ - Qwen2.5-3B

# åŸºäºSFTæ¨¡å‹è®­ç»ƒ
SFT_MODEL="outputs-sft-qwen2.5-3b/checkpoint-best"

CUDA_VISIBLE_DEVICES=0,1 python dpo_training.py \
    --model_name_or_path $SFT_MODEL \
    --template_name qwen \
    --train_file_dir ./data/reward \
    --validation_file_dir ./data/reward \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 16 \
    --per_device_eval_batch_size 1 \
    --do_train \
    --do_eval \
    --use_peft True \
    --max_train_samples 5000 \
    --max_eval_samples 100 \
    --max_steps 2000 \
    --eval_steps 100 \
    --save_steps 500 \
    --max_source_length 1024 \
    --max_target_length 512 \
    --learning_rate 5e-6 \
    --output_dir outputs-dpo-qwen2.5-3b \
    --target_modules all \
    --lora_rank 16 \
    --lora_alpha 32 \
    --lora_dropout 0.05 \
    --torch_dtype bfloat16 \
    --bf16 True \
    --fp16 False \
    --device_map auto \
    --report_to tensorboard \
    --remove_unused_columns False \
    --gradient_checkpointing True \
    --cache_dir ./cache
```

**å¯åŠ¨è®­ç»ƒï¼š**
```bash
chmod +x scripts/run_dpo_qwen2.5-3b.sh
nohup bash scripts/run_dpo_qwen2.5-3b.sh > logs/dpo_training.log 2>&1 &
```

### 4.4 é˜¶æ®µå››ï¼šPPOè®­ç»ƒ (é«˜çº§ï¼Œå¯é€‰)

**æ³¨æ„ï¼š** PPOè®­ç»ƒéå¸¸æ¶ˆè€—æ˜¾å­˜ï¼Œéœ€è¦ 2Ã—A100 (80GB)

**åˆ›å»ºè®­ç»ƒè„šæœ¬ï¼š** `scripts/run_ppo_qwen2.5-3b.sh`

```bash
#!/bin/bash
# PPOè®­ç»ƒ - Qwen2.5-3Bï¼ˆéœ€è¦å¤§æ˜¾å­˜ï¼‰

SFT_MODEL="outputs-sft-qwen2.5-3b/checkpoint-best"
RM_MODEL="outputs-rm-qwen2.5-3b/checkpoint-best"

CUDA_VISIBLE_DEVICES=0,1 python ppo_training.py \
    --sft_model_path $SFT_MODEL \
    --reward_model_path $RM_MODEL \
    --template_name qwen \
    --torch_dtype bfloat16 \
    --train_file_dir ./data/finetune \
    --validation_file_dir ./data/finetune \
    --max_source_length 1024 \
    --response_length 512 \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 8 \
    --gradient_checkpointing True \
    --do_train \
    --total_episodes 10000 \
    --output_dir outputs-ppo-qwen2.5-3b \
    --missing_eos_penalty 1.0 \
    --eval_strategy steps \
    --eval_steps 100 \
    --num_train_epochs 1 \
    --report_to tensorboard \
    --cache_dir ./cache
```

---

## 5. æ¨¡å‹è¯„ä¼°ä¸éƒ¨ç½²

### 5.1 æœ¬åœ°æµ‹è¯•

**åˆ›å»ºæµ‹è¯•è„šæœ¬ï¼š** `scripts/test_model.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•è®­ç»ƒå¥½çš„åŒ»ç–—æ¨¡å‹
"""
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

def load_model(base_model_path, lora_model_path=None):
    """åŠ è½½æ¨¡å‹"""
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_path,
        trust_remote_code=True
    )
    
    model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    if lora_model_path:
        print(f"Loading LoRA from {lora_model_path}")
        model = PeftModel.from_pretrained(model, lora_model_path)
        model = model.merge_and_unload()  # åˆå¹¶LoRAæƒé‡
    
    return model, tokenizer

def chat(model, tokenizer, query, history=[]):
    """å¯¹è¯å‡½æ•°"""
    # æ„å»ºprompt
    messages = history + [{"role": "user", "content": query}]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # Tokenize
    inputs = tokenizer([text], return_tensors="pt").to(model.device)
    
    # ç”Ÿæˆ
    outputs = model.generate(
        **inputs,
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.1,
        do_sample=True
    )
    
    response = tokenizer.decode(
        outputs[0][len(inputs.input_ids[0]):],
        skip_special_tokens=True
    )
    
    return response

def main():
    # é…ç½®
    base_model = "Qwen/Qwen2.5-3B-Instruct"
    lora_model = "outputs-dpo-qwen2.5-3b/checkpoint-best"  # ä½¿ç”¨DPOè®­ç»ƒåçš„æ¨¡å‹
    
    print("Loading model...")
    model, tokenizer = load_model(base_model, lora_model)
    
    print("\n" + "="*50)
    print("åŒ»ç–—é—®ç­”æ¨¡å‹æµ‹è¯•")
    print("="*50)
    
    # æµ‹è¯•é—®é¢˜
    test_questions = [
        "æ„Ÿå†’äº†åº”è¯¥æ€ä¹ˆåŠï¼Ÿ",
        "é«˜è¡€å‹æ‚£è€…åœ¨é¥®é£Ÿä¸Šéœ€è¦æ³¨æ„ä»€ä¹ˆï¼Ÿ",
        "ç³–å°¿ç—…æœ‰å“ªäº›æ—©æœŸç—‡çŠ¶ï¼Ÿ",
        "å¦‚ä½•é¢„é˜²å¿ƒè¡€ç®¡ç–¾ç—…ï¼Ÿ"
    ]
    
    for question in test_questions:
        print(f"\né—®é¢˜: {question}")
        print(f"å›ç­”: ", end="")
        response = chat(model, tokenizer, question)
        print(response)
        print("-" * 50)

if __name__ == "__main__":
    main()
```

**è¿è¡Œæµ‹è¯•ï¼š**
```bash
python scripts/test_model.py
```

### 5.2 ä½¿ç”¨ vLLM éƒ¨ç½²ï¼ˆé«˜æ€§èƒ½æ¨ç†ï¼‰

```bash
# å®‰è£… vLLM
pip install vllm

# åˆå¹¶LoRAæƒé‡ï¼ˆå¦‚æœè¿˜æ²¡åˆå¹¶ï¼‰
python scripts/merge_lora.py \
    --base_model Qwen/Qwen2.5-3B-Instruct \
    --lora_model outputs-dpo-qwen2.5-3b/checkpoint-best \
    --output_dir outputs-dpo-qwen2.5-3b-merged

# å¯åŠ¨ vLLM æœåŠ¡
python -m vllm.entrypoints.openai.api_server \
    --model outputs-dpo-qwen2.5-3b-merged \
    --host 0.0.0.0 \
    --port 8000 \
    --dtype bfloat16 \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.9

# æµ‹è¯•API
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "outputs-dpo-qwen2.5-3b-merged",
    "messages": [{"role": "user", "content": "æ„Ÿå†’äº†æ€ä¹ˆåŠï¼Ÿ"}],
    "temperature": 0.7,
    "max_tokens": 512
  }'
```

### 5.3 æ¨¡å‹è¯„ä¼°

**åˆ›å»ºè¯„ä¼°è„šæœ¬ï¼š** `scripts/evaluate_model.py`

```python
#!/usr/bin/env python3
"""
è¯„ä¼°æ¨¡å‹åœ¨åŒ»ç–—é—®ç­”ä»»åŠ¡ä¸Šçš„è¡¨ç°
"""
import json
from tqdm import tqdm
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

def evaluate(model, tokenizer, test_file, num_samples=100):
    """è¯„ä¼°æ¨¡å‹"""
    # åŠ è½½æµ‹è¯•æ•°æ®
    with open(test_file, 'r', encoding='utf-8') as f:
        test_data = [json.loads(line) for line in f][:num_samples]
    
    total = 0
    correct = 0
    
    for item in tqdm(test_data):
        question = item['conversations'][0]['value']
        reference = item['conversations'][1]['value']
        
        # ç”Ÿæˆå›ç­”
        messages = [{"role": "user", "content": question}]
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = tokenizer([text], return_tensors="pt").to(model.device)
        
        outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.7)
        response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
        
        # ç®€å•è¯„ä¼°ï¼ˆå®é™…åº”è¯¥ç”¨æ›´å¤æ‚çš„æŒ‡æ ‡ï¼‰
        print(f"\nQ: {question}")
        print(f"A: {response}")
        print(f"Ref: {reference}")
        
        total += 1
    
    print(f"\nEvaluated {total} samples")

if __name__ == "__main__":
    # é…ç½®
    base_model = "Qwen/Qwen2.5-3B-Instruct"
    lora_model = "outputs-dpo-qwen2.5-3b/checkpoint-best"
    test_file = "data/finetune/medical_sft_1K_format.jsonl"
    
    # åŠ è½½æ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        base_model, torch_dtype=torch.bfloat16, device_map="auto", trust_remote_code=True
    )
    model = PeftModel.from_pretrained(model, lora_model)
    
    # è¯„ä¼°
    evaluate(model, tokenizer, test_file, num_samples=10)
```

---

## 6. å¸¸è§é—®é¢˜ä¸ä¼˜åŒ–

### 6.1 æ˜¾å­˜ä¸è¶³ (OOM)

**é—®é¢˜ï¼š** `CUDA out of memory`

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# 1. å‡å°batch size
--per_device_train_batch_size 1
--gradient_accumulation_steps 32  # å¢å¤§ç´¯ç§¯æ­¥æ•°

# 2. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
--gradient_checkpointing True

# 3. ä½¿ç”¨8bité‡åŒ–
--load_in_8bit True

# 4. å‡å°æ¨¡å‹åºåˆ—é•¿åº¦
--model_max_length 1024  # ä»2048é™åˆ°1024

# 5. å‡å°LoRA rank
--lora_rank 8  # ä»16é™åˆ°8
```

### 6.2 è®­ç»ƒé€Ÿåº¦æ…¢

**ä¼˜åŒ–å»ºè®®ï¼š**
```bash
# 1. å¯ç”¨FlashAttention
--flash_attn True

# 2. å¯ç”¨bf16æ··åˆç²¾åº¦
--bf16 True
--torch_dtype bfloat16

# 3. å¢å¤§batch sizeï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰
--per_device_train_batch_size 4

# 4. å‡å°‘evalé¢‘ç‡
--eval_steps 1000  # ä»100å¢åŠ åˆ°1000

# 5. å¯ç”¨torchç¼–è¯‘ï¼ˆPyTorch 2.0+ï¼‰
export TORCH_COMPILE=1
```

### 6.3 è®­ç»ƒä¸ç¨³å®š/Lossä¸ä¸‹é™

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# 1. è°ƒæ•´å­¦ä¹ ç‡
--learning_rate 1e-5  # å°è¯•æ›´å°çš„å­¦ä¹ ç‡

# 2. å¢åŠ warmup
--warmup_ratio 0.1  # ä»0.05å¢åŠ åˆ°0.1

# 3. è°ƒæ•´weight decay
--weight_decay 0.01

# 4. æ£€æŸ¥æ•°æ®è´¨é‡
# ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®ï¼Œæ²¡æœ‰å¼‚å¸¸æ ·æœ¬

# 5. ä½¿ç”¨æ›´ç¨³å®šçš„ä¼˜åŒ–å™¨
--optim adamw_torch  # æˆ– adafactor
```

### 6.4 æ•°æ®åŠ è½½æ…¢

**ä¼˜åŒ–å»ºè®®ï¼š**
```bash
# 1. å¢åŠ æ•°æ®å¤„ç†workers
--preprocessing_num_workers 16

# 2. ä½¿ç”¨æ•°æ®ç¼“å­˜
# ç¬¬äºŒæ¬¡è®­ç»ƒä¼šè‡ªåŠ¨ä½¿ç”¨ç¼“å­˜ï¼Œé™¤é:
--overwrite_cache  # åˆ é™¤è¿™ä¸ªå‚æ•°

# 3. é¢„å¤„ç†æ•°æ®
python scripts/preprocess_data.py \
    --input_dir data/finetune \
    --output_dir data/finetune_processed
```

### 6.5 ç›‘æ§è®­ç»ƒè¿›åº¦

**ä½¿ç”¨ TensorBoardï¼š**
```bash
# å¯åŠ¨TensorBoardï¼ˆåœ¨æœ¬åœ°ï¼‰
tensorboard --logdir outputs-sft-qwen2.5-3b --port 6006 --bind_all

# å¦‚æœæ˜¯è¿œç¨‹æœåŠ¡å™¨ï¼Œéœ€è¦ç«¯å£è½¬å‘
# åœ¨æœ¬åœ°ç”µè„‘è¿è¡Œï¼š
ssh -L 6006:localhost:6006 user@server_ip

# ç„¶ååœ¨æµè§ˆå™¨è®¿é—®ï¼š
http://localhost:6006
```

**ä½¿ç”¨ wandbï¼š**
```bash
# å®‰è£…wandb
pip install wandb

# ç™»å½•
wandb login

# ä¿®æ”¹è®­ç»ƒè„šæœ¬
--report_to wandb
--run_name medical-gpt-sft-qwen2.5-3b
```

### 6.6 æ–­ç‚¹ç»­è®­

```bash
# è®­ç»ƒä¸­æ–­åï¼Œä»æœ€æ–°checkpointç»§ç»­
--resume_from_checkpoint outputs-sft-qwen2.5-3b/checkpoint-500

# æˆ–è®©ç¨‹åºè‡ªåŠ¨æ‰¾æœ€æ–°checkpoint
--resume_from_checkpoint True
```

---

## 7. å®Œæ•´è®­ç»ƒæ—¶é—´è¡¨

### å‡è®¾é…ç½®ï¼š2Ã—A100 (40GB)

| é˜¶æ®µ | æ•°æ®é‡ | é¢„è®¡æ—¶é—´ | è¾“å‡º |
|-----|-------|---------|------|
| **PT** | 10ä¸‡æ¡æ–‡æœ¬ | 12-18å°æ—¶ | `outputs-pt-qwen2.5-3b` |
| **SFT** | 1ä¸‡æ¡å¯¹è¯ | 4-6å°æ—¶ | `outputs-sft-qwen2.5-3b` |
| **RM** | 5åƒæ¡åå¥½ | 2-3å°æ—¶ | `outputs-rm-qwen2.5-3b` |
| **DPO** | 5åƒæ¡åå¥½ | 4-6å°æ—¶ | `outputs-dpo-qwen2.5-3b` |
| **æ€»è®¡** | - | **22-33å°æ—¶** | - |

### é¢„ç®—ä¼°ç®—ï¼ˆAutoDLï¼Œ2Ã—A100 40GBï¼‰

- **å•ä»·ï¼š** ~18å…ƒ/å°æ—¶
- **æ€»æ—¶é•¿ï¼š** 30å°æ—¶
- **æ€»è´¹ç”¨ï¼š** ~540å…ƒ

---

## 8. å¿«é€Ÿå¼€å§‹è„šæœ¬

**åˆ›å»ºä¸€é”®è®­ç»ƒè„šæœ¬ï¼š** `scripts/train_all.sh`

```bash
#!/bin/bash
# ä¸€é”®å®Œæˆæ‰€æœ‰è®­ç»ƒé˜¶æ®µ

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

echo "========================================="
echo "MedicalGPT å®Œæ•´è®­ç»ƒæµç¨‹"
echo "æ¨¡å‹: Qwen2.5-3B"
echo "========================================="

# åˆ›å»ºå¿…è¦ç›®å½•
mkdir -p logs outputs cache scripts

# é˜¶æ®µ1: é¢„è®­ç»ƒï¼ˆå¯é€‰ï¼Œè·³è¿‡åˆ™ä»SFTå¼€å§‹ï¼‰
read -p "æ˜¯å¦è¿›è¡Œé¢„è®­ç»ƒ? (y/n): " do_pt
if [ "$do_pt" = "y" ]; then
    echo "\n[1/4] å¼€å§‹é¢„è®­ç»ƒ..."
    nohup bash scripts/run_pt_qwen2.5-3b.sh > logs/pt.log 2>&1
    echo "é¢„è®­ç»ƒå®Œæˆï¼"
fi

# é˜¶æ®µ2: ç›‘ç£å¾®è°ƒ
echo "\n[2/4] å¼€å§‹ç›‘ç£å¾®è°ƒ..."
nohup bash scripts/run_sft_qwen2.5-3b.sh > logs/sft.log 2>&1
echo "ç›‘ç£å¾®è°ƒå®Œæˆï¼"

# é˜¶æ®µ3: é€‰æ‹©RM+PPO æˆ– DPO
echo "\né€‰æ‹©å¼ºåŒ–å­¦ä¹ æ–¹æ³•:"
echo "1) DPO (æ¨èï¼Œæ›´ç®€å•)"
echo "2) RM + PPO (å¤æ‚ï¼Œéœ€è¦å¤§æ˜¾å­˜)"
read -p "è¯·é€‰æ‹© (1/2): " rl_method

if [ "$rl_method" = "1" ]; then
    echo "\n[3/4] å¼€å§‹DPOè®­ç»ƒ..."
    nohup bash scripts/run_dpo_qwen2.5-3b.sh > logs/dpo.log 2>&1
    echo "DPOè®­ç»ƒå®Œæˆï¼"
else
    echo "\n[3/4] å¼€å§‹RMè®­ç»ƒ..."
    nohup bash scripts/run_rm_qwen2.5-3b.sh > logs/rm.log 2>&1
    echo "RMè®­ç»ƒå®Œæˆï¼"
    
    echo "\n[4/4] å¼€å§‹PPOè®­ç»ƒ..."
    nohup bash scripts/run_ppo_qwen2.5-3b.sh > logs/ppo.log 2>&1
    echo "PPOè®­ç»ƒå®Œæˆï¼"
fi

echo "\n========================================="
echo "æ‰€æœ‰è®­ç»ƒå®Œæˆï¼"
echo "æ¨¡å‹ä¿å­˜åœ¨: outputs-*-qwen2.5-3b/"
echo "========================================="
```

**ä½¿ç”¨æ–¹æ³•ï¼š**
```bash
chmod +x scripts/train_all.sh
bash scripts/train_all.sh
```

---

## 9. é™„å½•

### 9.1 æ•°æ®æ ¼å¼è½¬æ¢å·¥å…·

**åˆ›å»ºï¼š** `scripts/convert_data_format.py`

```python
#!/usr/bin/env python3
"""
æ•°æ®æ ¼å¼è½¬æ¢å·¥å…·
"""
import json
import argparse

def convert_to_sft_format(input_file, output_file):
    """è½¬æ¢ä¸ºSFTæ ¼å¼"""
    with open(input_file, 'r', encoding='utf-8') as f_in, \
         open(output_file, 'w', encoding='utf-8') as f_out:
        for line in f_in:
            data = json.loads(line)
            
            # å‡è®¾åŸæ ¼å¼æ˜¯ {"question": "...", "answer": "..."}
            sft_data = {
                "conversations": [
                    {"from": "human", "value": data["question"]},
                    {"from": "gpt", "value": data["answer"]}
                ]
            }
            
            f_out.write(json.dumps(sft_data, ensure_ascii=False) + '\n')

def convert_to_dpo_format(input_file, output_file):
    """è½¬æ¢ä¸ºDPOæ ¼å¼"""
    # å‡è®¾åŸæ ¼å¼æœ‰ question, good_answer, bad_answer
    with open(input_file, 'r', encoding='utf-8') as f_in, \
         open(output_file, 'w', encoding='utf-8') as f_out:
        for line in f_in:
            data = json.loads(line)
            
            dpo_data = {
                "system": "",
                "history": [],
                "question": data["question"],
                "response_chosen": data["good_answer"],
                "response_rejected": data["bad_answer"]
            }
            
            f_out.write(json.dumps(dpo_data, ensure_ascii=False) + '\n')

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", required=True, help="è¾“å…¥æ–‡ä»¶")
    parser.add_argument("--output", required=True, help="è¾“å‡ºæ–‡ä»¶")
    parser.add_argument("--format", choices=["sft", "dpo"], required=True)
    
    args = parser.parse_args()
    
    if args.format == "sft":
        convert_to_sft_format(args.input, args.output)
    else:
        convert_to_dpo_format(args.input, args.output)
    
    print(f"è½¬æ¢å®Œæˆ: {args.output}")
```

### 9.2 æ¨¡å‹åˆå¹¶å·¥å…·

**åˆ›å»ºï¼š** `scripts/merge_lora.py`

```python
#!/usr/bin/env python3
"""
åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹
"""
import argparse
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

def merge_lora(base_model_path, lora_model_path, output_dir):
    """åˆå¹¶LoRAæƒé‡"""
    print(f"Loading base model from {base_model_path}")
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_path,
        trust_remote_code=True
    )
    
    model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    print(f"Loading LoRA from {lora_model_path}")
    model = PeftModel.from_pretrained(model, lora_model_path)
    
    print("Merging LoRA weights...")
    model = model.merge_and_unload()
    
    print(f"Saving merged model to {output_dir}")
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    print("Done!")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--base_model", required=True, help="åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--lora_model", required=True, help="LoRAæ¨¡å‹è·¯å¾„")
    parser.add_argument("--output_dir", required=True, help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    merge_lora(args.base_model, args.lora_model, args.output_dir)
```

### 9.3 æœ‰ç”¨çš„å‘½ä»¤

```bash
# æŸ¥çœ‹GPUä½¿ç”¨æƒ…å†µ
watch -n 1 nvidia-smi

# æŸ¥çœ‹è¿›ç¨‹
ps aux | grep python

# æ€æ­»è®­ç»ƒè¿›ç¨‹
pkill -f "pretraining.py"

# æŸ¥çœ‹ç£ç›˜ç©ºé—´
df -h

# å‹ç¼©æ¨¡å‹æ–‡ä»¶
tar -czf outputs-sft-qwen2.5-3b.tar.gz outputs-sft-qwen2.5-3b/

# ä¸‹è½½åˆ°æœ¬åœ°ï¼ˆåœ¨æœ¬åœ°è¿è¡Œï¼‰
scp -r user@server:/root/MedicalGPT/outputs-sft-qwen2.5-3b ./

# æ¸…ç†ç¼“å­˜
rm -rf cache/
rm -rf ~/.cache/huggingface/
```

---

## 10. æ€»ç»“ä¸å»ºè®®

### æ¨èè®­ç»ƒè·¯å¾„

1. **å¿«é€ŸéªŒè¯è·¯å¾„**ï¼ˆ1-2å¤©ï¼‰ï¼š
   ```
   SFT (1ä¸‡æ¡) â†’ DPO (5åƒæ¡) â†’ éƒ¨ç½²æµ‹è¯•
   ```

2. **å®Œæ•´è®­ç»ƒè·¯å¾„**ï¼ˆ3-5å¤©ï¼‰ï¼š
   ```
   PT (10ä¸‡æ¡) â†’ SFT (5ä¸‡æ¡) â†’ DPO (1ä¸‡æ¡) â†’ éƒ¨ç½²
   ```

3. **é«˜çº§è·¯å¾„**ï¼ˆéœ€å¤§æ˜¾å­˜ï¼‰ï¼š
   ```
   PT â†’ SFT â†’ RM â†’ PPO â†’ éƒ¨ç½²
   ```

### å…³é”®æç¤º

- âœ… **ä»SFTå¼€å§‹**ï¼šå¦‚æœæ²¡æœ‰å¤§é‡é¢†åŸŸæ–‡æœ¬ï¼Œè·³è¿‡PTç›´æ¥SFT
- âœ… **ä¼˜å…ˆä½¿ç”¨DPO**ï¼šæ¯”PPOæ›´ç®€å•ï¼Œæ•ˆæœç›¸å½“
- âœ… **æ•°æ®è´¨é‡>æ•°é‡**ï¼š1ä¸‡æ¡é«˜è´¨é‡æ•°æ®ä¼˜äº10ä¸‡æ¡ä½è´¨é‡æ•°æ®
- âœ… **å®šæœŸä¿å­˜checkpoint**ï¼šé¿å…è®­ç»ƒä¸­æ–­å¯¼è‡´é‡æ¥
- âœ… **ç›‘æ§æ˜¾å­˜ä½¿ç”¨**ï¼šåŠæ—¶è°ƒæ•´batch sizeé¿å…OOM

### è·å–å¸®åŠ©

- é¡¹ç›®GitHub: https://github.com/shibing624/MedicalGPT
- Issues: https://github.com/shibing624/MedicalGPT/issues
- Qwenæ–‡æ¡£: https://qwenlm.github.io/

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸ‰**
