# æœ¬åœ°æ•°æ®å‡†å¤‡ + æœåŠ¡å™¨è®­ç»ƒæ–¹æ¡ˆ

> åœ¨æœ¬åœ°å®Œæˆæ•°æ®å‡†å¤‡å·¥ä½œï¼Œé€šè¿‡ Git + æ–‡ä»¶ä¼ è¾“ä¸Šä¼ åˆ°æœåŠ¡å™¨è®­ç»ƒ

---

## ğŸ“‹ ä»»åŠ¡åˆ’åˆ†

### âœ… æœ¬åœ°å®Œæˆï¼ˆæ•°æ®å‡†å¤‡ï¼‰

| æ­¥éª¤ | ä»»åŠ¡ | è€—æ—¶ | è¯´æ˜ |
|------|------|------|------|
| 1 | ä¸‹è½½è¯„æµ‹é›† | 5åˆ†é’Ÿ | æ–‡ä»¶å°ï¼Œå¯ç”¨GitåŒæ­¥ |
| 2 | å‘é‡åŒ–è¯„æµ‹é›† | 5-10åˆ†é’Ÿ | æ–‡ä»¶çº¦20MBï¼Œå¯ç”¨Git |
| 3 | å‘é‡åŒ–è®­ç»ƒæ•°æ® | 6-12å°æ—¶ | **æ–‡ä»¶å¤§ï¼ˆ2-5GBï¼‰**ï¼Œéœ€å•ç‹¬ä¼ è¾“ |
| 4 | å¬å›æ•°æ® | 30åˆ†é’Ÿ | ä¾èµ–æ­¥éª¤3 |
| 5 | åˆå¹¶è®­ç»ƒé›† | 5åˆ†é’Ÿ | æœ€ç»ˆæ–‡ä»¶çº¦50MB |

### ğŸš€ æœåŠ¡å™¨å®Œæˆï¼ˆè®­ç»ƒï¼‰

| æ­¥éª¤ | ä»»åŠ¡ | è€—æ—¶ | è¯´æ˜ |
|------|------|------|------|
| 6 | SFTè®­ç»ƒ | 12-24å°æ—¶ | éœ€è¦GPU |
| 7 | æ¨¡å‹è¯„æµ‹ | 1-2å°æ—¶ | å¯é€‰ |
| 8 | DPO/PPOè®­ç»ƒ | 12-24å°æ—¶ | å¯é€‰ |

---

## ğŸ”§ æœ¬åœ°å‡†å¤‡æµç¨‹

### 1. ç¯å¢ƒå‡†å¤‡

```powershell
# Windows PowerShell

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼ˆå¦‚æœæœ‰ï¼‰
# .\venv_medical\Scripts\Activate.ps1

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
pip install -r requirements_eval_driven.txt

# è®¾ç½®ç¯å¢ƒå˜é‡
$env:ZHIPUAI_API_KEY="5fbd8d5375e54946884bd2d796a9c12a.CzJhYueAB8LV729i"
$env:HF_ENDPOINT="https://hf-mirror.com"
```

### 2. æ‰§è¡Œæœ¬åœ°å‡†å¤‡è„šæœ¬

```powershell
# è¿è¡Œæœ¬åœ°å‡†å¤‡è„šæœ¬ï¼ˆWindowsç‰ˆæœ¬ï¼‰
python scripts/local_prepare.py
```

è¿™ä¸ªè„šæœ¬ä¼šè‡ªåŠ¨å®Œæˆï¼š
- âœ… ä¸‹è½½è¯„æµ‹é›†
- âœ… å‘é‡åŒ–è¯„æµ‹é›†
- âœ… å‘é‡åŒ–è®­ç»ƒæ•°æ®ï¼ˆå¯é€‰æ•°é‡ï¼‰
- âœ… å¬å›æ•°æ®
- âœ… åˆå¹¶è®­ç»ƒé›†

---

## ğŸ“¤ æ•°æ®ä¼ è¾“æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: Git + é˜¿é‡Œäº‘OSS/ä¸ƒç‰›äº‘ï¼ˆæ¨èï¼‰

#### Git åŒæ­¥ï¼ˆå°æ–‡ä»¶ï¼‰

```bash
# 1. æ·»åŠ å°æ–‡ä»¶åˆ° Git
git add data/eval_benchmark/
git add data/eval_vectorized/
git add data/recalled_data/
git add data/finetune/medical_eval_driven.jsonl
git commit -m "Add prepared training data"
git push
```

#### å¯¹è±¡å­˜å‚¨ä¼ è¾“ï¼ˆå¤§æ–‡ä»¶ï¼‰

```powershell
# ä½¿ç”¨ ossutil ä¸Šä¼ å¤§æ–‡ä»¶ï¼ˆ2-5GBçš„å‘é‡æ–‡ä»¶ï¼‰

# å®‰è£… ossutilï¼ˆWindowsï¼‰
# ä¸‹è½½: https://help.aliyun.com/document_detail/120075.html

# é…ç½®
ossutil config

# ä¸Šä¼ å‘é‡æ–‡ä»¶
ossutil cp -r data\train_vectorized\ oss://your-bucket/medicalgpt/train_vectorized/

# åœ¨æœåŠ¡å™¨ä¸‹è½½
# ossutil cp -r oss://your-bucket/medicalgpt/train_vectorized/ data/train_vectorized/
```

### æ–¹æ¡ˆ2: Git + SCP/SFTP

```bash
# 1. Git åŒæ­¥å°æ–‡ä»¶
git push

# 2. SCP ä¼ è¾“å¤§æ–‡ä»¶ï¼ˆä»æœ¬åœ°åˆ°æœåŠ¡å™¨ï¼‰
scp -r data/train_vectorized/ root@your-server-ip:/root/MedicalGPT/data/

# æˆ–ä½¿ç”¨ WinSCPï¼ˆWindowså›¾å½¢ç•Œé¢å·¥å…·ï¼‰
# ä¸‹è½½: https://winscp.net/
```

### æ–¹æ¡ˆ3: Git + ç½‘ç›˜ä¸­è½¬

```bash
# 1. å‹ç¼©å¤§æ–‡ä»¶
tar -czf train_vectorized.tar.gz data/train_vectorized/

# 2. ä¸Šä¼ åˆ°ç™¾åº¦ç½‘ç›˜/é˜¿é‡Œäº‘ç›˜/åšæœäº‘

# 3. åœ¨æœåŠ¡å™¨ä¸‹è½½å¹¶è§£å‹
wget "ç½‘ç›˜åˆ†äº«é“¾æ¥" -O train_vectorized.tar.gz
tar -xzf train_vectorized.tar.gz -C data/
```

### æ–¹æ¡ˆ4: ç›´æ¥åœ¨æœåŠ¡å™¨å‘é‡åŒ–ï¼ˆå¤‡é€‰ï¼‰

å¦‚æœä¼ è¾“å›°éš¾ï¼Œå¯ä»¥åªä¼ ä»£ç ï¼Œåœ¨æœåŠ¡å™¨é‡æ–°å‘é‡åŒ–ï¼š

```bash
# æœåŠ¡å™¨æ‰§è¡Œ
python scripts/vectorize_training_dataset.py \
    --dataset_name shibing624/medical \
    --output_file data/train_vectorized/medical_vectorized.jsonl \
    --max_samples 500000
```

---

## ğŸ“ Git é…ç½®å»ºè®®

### .gitignore é…ç½®

åˆ›å»º/æ›´æ–° `.gitignore`ï¼Œæ’é™¤å¤§æ–‡ä»¶ï¼š

```bash
# å¤§æ–‡ä»¶ä¸æäº¤åˆ° Git
data/train_vectorized/*.jsonl
*.tar.gz
*.zip

# æ¨¡å‹æƒé‡ä¸æäº¤
outputs-*/
*.bin
*.safetensors

# å…¶ä»–
__pycache__/
*.pyc
.DS_Store
```

### Git LFSï¼ˆå¯é€‰ï¼Œé€‚åˆä¸­ç­‰æ–‡ä»¶ï¼‰

å¦‚æœæƒ³ç”¨ Git ç®¡ç† 50-200MB çš„æ–‡ä»¶ï¼š

```bash
# å®‰è£… Git LFS
# Windows: https://git-lfs.github.com/

# é…ç½® Git LFS
git lfs install

# è¿½è¸ªç‰¹å®šæ–‡ä»¶
git lfs track "data/finetune/*.jsonl"
git lfs track "data/recalled_data/*.jsonl"

# æäº¤
git add .gitattributes
git add data/finetune/
git commit -m "Add training data with LFS"
git push
```

---

## ğŸ–¥ï¸ æœåŠ¡å™¨æ¥æ”¶æ•°æ®

### æ–¹æ¡ˆA: Gitå…‹éš†

```bash
# 1. SSH è¿æ¥åˆ°æœåŠ¡å™¨
ssh root@your-server-ip

# 2. å…‹éš†ä»£ç 
cd /root
git clone https://github.com/yourusername/MedicalGPT.git
cd MedicalGPT

# 3. ä¸‹è½½å¤§æ–‡ä»¶ï¼ˆå¦‚æœä½¿ç”¨ OSSï¼‰
ossutil cp -r oss://your-bucket/medicalgpt/train_vectorized/ data/train_vectorized/

# 4. éªŒè¯æ–‡ä»¶
ls -lh data/train_vectorized/
ls -lh data/finetune/
```

### æ–¹æ¡ˆB: rsync åŒæ­¥ï¼ˆå¢é‡ä¼ è¾“ï¼‰

```bash
# ä»æœ¬åœ°åŒæ­¥åˆ°æœåŠ¡å™¨ï¼ˆå¢é‡ï¼Œæ–­ç‚¹ç»­ä¼ ï¼‰
rsync -avz --progress \
    data/ \
    root@your-server-ip:/root/MedicalGPT/data/

# åªåŒæ­¥ç‰¹å®šç›®å½•
rsync -avz --progress \
    data/train_vectorized/ \
    root@your-server-ip:/root/MedicalGPT/data/train_vectorized/
```

---

## ğŸ“ å®Œæ•´æ“ä½œæ­¥éª¤

### é˜¶æ®µ1: æœ¬åœ°å‡†å¤‡ï¼ˆWindowsï¼‰

```powershell
# 1. è®¾ç½®ç¯å¢ƒ
$env:ZHIPUAI_API_KEY="your_api_key"
$env:HF_ENDPOINT="https://hf-mirror.com"

# 2. æ‰§è¡Œå‡†å¤‡è„šæœ¬
python scripts/local_prepare.py --max_samples 100000

# 3. éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶
Get-ChildItem -Recurse data\ | Select-Object FullName, Length

# 4. æäº¤å°æ–‡ä»¶åˆ° Git
git add data/eval_benchmark/
git add data/eval_vectorized/
git add data/recalled_data/
git add data/finetune/
git commit -m "Prepare training data"
git push

# 5. å‹ç¼©å¤§æ–‡ä»¶å‡†å¤‡ä¼ è¾“
Compress-Archive -Path data\train_vectorized -DestinationPath train_vectorized.zip
```

### é˜¶æ®µ2: ä¼ è¾“åˆ°æœåŠ¡å™¨

**æ–¹å¼1: ä½¿ç”¨ WinSCPï¼ˆæ¨èï¼‰**
- ä¸‹è½½ WinSCP: https://winscp.net/
- è¿æ¥æœåŠ¡å™¨ï¼Œæ‹–æ‹½ä¸Šä¼  `train_vectorized.zip`

**æ–¹å¼2: ä½¿ç”¨å‘½ä»¤è¡Œ**
```powershell
# SCP ä¸Šä¼ ï¼ˆéœ€è¦ OpenSSHï¼‰
scp train_vectorized.zip root@your-server-ip:/root/
```

### é˜¶æ®µ3: æœåŠ¡å™¨è®­ç»ƒï¼ˆLinuxï¼‰

```bash
# 1. SSH è¿æ¥
ssh root@your-server-ip

# 2. æ‹‰å–ä»£ç ï¼ˆå¦‚æœç”¨Gitï¼‰
cd /root/MedicalGPT
git pull

# 3. è§£å‹å¤§æ–‡ä»¶
unzip /root/train_vectorized.zip -d data/

# 4. éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
python scripts/verify_data.py

# 5. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 6. å¼€å§‹è®­ç»ƒ
bash scripts/run_sft_eval_driven.sh
```

---

## âš™ï¸ è‡ªåŠ¨åŒ–è„šæœ¬

### æœ¬åœ°å‡†å¤‡è„šæœ¬ï¼ˆWindowsï¼‰

åˆ›å»º `scripts/local_prepare.py`ï¼ˆå·²åŒ…å«åœ¨å‰é¢åˆ›å»ºçš„æ–‡ä»¶ä¸­ï¼‰

### å¿«é€Ÿå‘½ä»¤ï¼ˆPowerShellï¼‰

```powershell
# ä¿å­˜ä¸º local_prepare.ps1
$env:ZHIPUAI_API_KEY="your_api_key"
$env:HF_ENDPOINT="https://hf-mirror.com"

Write-Host "Step 1: ä¸‹è½½è¯„æµ‹é›†" -ForegroundColor Green
python scripts/download_ceval.py

Write-Host "Step 2: å‘é‡åŒ–è¯„æµ‹é›†" -ForegroundColor Green
python scripts/vectorize_eval_dataset.py `
    --input_dir data/eval_benchmark `
    --output_dir data/eval_vectorized `
    --model_name glm-embedding-3

Write-Host "Step 3: å‘é‡åŒ–è®­ç»ƒæ•°æ®ï¼ˆå¯èƒ½éœ€è¦6-12å°æ—¶ï¼‰" -ForegroundColor Yellow
python scripts/vectorize_training_dataset.py `
    --dataset_name shibing624/medical `
    --output_file data/train_vectorized/medical_vectorized.jsonl `
    --max_samples 100000

Write-Host "Step 4: å¬å›æ•°æ®" -ForegroundColor Green
python scripts/recall_relevant_data.py `
    --eval_vectors data/eval_vectorized `
    --train_vectors data/train_vectorized/medical_vectorized.jsonl `
    --output_dir data/recalled_data

Write-Host "Step 5: åˆå¹¶æ•°æ®" -ForegroundColor Green
python scripts/merge_recalled_data.py `
    --input_dir data/recalled_data `
    --output_file data/finetune/medical_eval_driven.jsonl `
    --format sharegpt

Write-Host "âœ… æœ¬åœ°å‡†å¤‡å®Œæˆï¼" -ForegroundColor Cyan
Write-Host "ä¸‹ä¸€æ­¥: ä¼ è¾“æ•°æ®åˆ°æœåŠ¡å™¨" -ForegroundColor Yellow
```

---

## ğŸ“Š æ–‡ä»¶å¤§å°å‚è€ƒ

| æ–‡ä»¶/ç›®å½• | å¤§å° | GitåŒæ­¥ | ä¼ è¾“æ–¹å¼ |
|----------|------|---------|---------|
| `data/eval_benchmark/` | ~200KB | âœ… æ˜¯ | Git |
| `data/eval_vectorized/` | ~20MB | âœ… æ˜¯ | Git |
| `data/train_vectorized/` | **2-5GB** | âŒ å¦ | OSS/SCP/ç½‘ç›˜ |
| `data/recalled_data/` | ~50MB | âœ… æ˜¯ | Git |
| `data/finetune/*.jsonl` | ~50MB | âœ… æ˜¯ | Git |

**æ€»è®¡**:
- Gitç®¡ç†: ~120MB
- å•ç‹¬ä¼ è¾“: 2-5GB

---

## ğŸ” æ•°æ®éªŒè¯è„šæœ¬

åˆ›å»º `scripts/verify_data.py`ï¼š

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""éªŒè¯æ•°æ®å®Œæ•´æ€§"""

import json
from pathlib import Path

def verify_data():
    """éªŒè¯æ‰€æœ‰å¿…éœ€çš„æ•°æ®æ–‡ä»¶"""
    
    required_files = [
        "data/eval_benchmark/clinical_medicine.jsonl",
        "data/eval_vectorized/clinical_medicine_vectorized.jsonl",
        "data/train_vectorized/medical_vectorized.jsonl",
        "data/recalled_data/recalled_clinical_medicine.jsonl",
        "data/finetune/medical_eval_driven.jsonl"
    ]
    
    print("éªŒè¯æ•°æ®æ–‡ä»¶å®Œæ•´æ€§...")
    print("=" * 60)
    
    all_ok = True
    
    for file_path in required_files:
        path = Path(file_path)
        if path.exists():
            size = path.stat().st_size / 1024 / 1024  # MB
            
            # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„JSONL
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    lines = sum(1 for _ in f)
                print(f"âœ… {file_path}")
                print(f"   å¤§å°: {size:.2f} MB, è¡Œæ•°: {lines}")
            except Exception as e:
                print(f"âš ï¸  {file_path} - æ–‡ä»¶æŸå: {e}")
                all_ok = False
        else:
            print(f"âŒ {file_path} - æ–‡ä»¶ä¸å­˜åœ¨")
            all_ok = False
    
    print("=" * 60)
    if all_ok:
        print("âœ… æ‰€æœ‰æ–‡ä»¶éªŒè¯é€šè¿‡ï¼å¯ä»¥å¼€å§‹è®­ç»ƒã€‚")
    else:
        print("âŒ éƒ¨åˆ†æ–‡ä»¶ç¼ºå¤±æˆ–æŸåï¼Œè¯·æ£€æŸ¥ã€‚")
    
    return all_ok

if __name__ == "__main__":
    verify_data()
```

---

## ğŸ’¡ æœ€ä½³å®è·µå»ºè®®

### 1. åˆ†æ‰¹å‡†å¤‡ï¼ˆæ¨èï¼‰

```bash
# ç¬¬ä¸€æ‰¹: å…ˆç”¨å°æ•°æ®æµ‹è¯•æµç¨‹
python scripts/local_prepare.py --max_samples 10000

# éªŒè¯æµç¨‹æ­£ç¡®åï¼Œå†å¤„ç†å®Œæ•´æ•°æ®
python scripts/local_prepare.py --max_samples 500000
```

### 2. æ–­ç‚¹ç»­ä¼ 

```python
# å‘é‡åŒ–æ”¯æŒæ–­ç‚¹ç»­ä¼ 
python scripts/vectorize_training_dataset.py \
    --dataset_name shibing624/medical \
    --output_file data/train_vectorized/medical_vectorized.jsonl \
    --max_samples 500000 \
    --resume_from data/train_vectorized/medical_vectorized.jsonl  # ä»å·²æœ‰æ–‡ä»¶ç»§ç»­
```

### 3. å‹ç¼©ä¼ è¾“

```bash
# å‹ç¼©å¯å‡å°‘50-70%æ–‡ä»¶å¤§å°
tar -czf data_prepared.tar.gz data/

# æœåŠ¡å™¨è§£å‹
tar -xzf data_prepared.tar.gz
```

### 4. æ ¡éªŒæ–‡ä»¶å®Œæ•´æ€§

```bash
# æœ¬åœ°ç”ŸæˆMD5
Get-FileHash -Path train_vectorized.zip -Algorithm MD5

# æœåŠ¡å™¨éªŒè¯
md5sum train_vectorized.zip
```

---

## ğŸš¨ å¸¸è§é—®é¢˜

### Q1: å‘é‡åŒ–åœ¨æœ¬åœ°å¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ

A: 3ç§æ–¹æ¡ˆï¼š
1. **è¿‡å¤œè¿è¡Œ**: ç¡å‰å¯åŠ¨ï¼Œæ—©ä¸Šå®Œæˆ
2. **ä½¿ç”¨æœ¬åœ°æ¨¡å‹**: ä¸éœ€è¦APIï¼Œé€Ÿåº¦æ›´å¿«
3. **å‡å°‘æ ·æœ¬æ•°**: å…ˆç”¨10ä¸‡æ¡æµ‹è¯•æ•ˆæœ

### Q2: ç½‘ç»œä¼ è¾“å¤ªæ…¢ï¼Ÿ

A: ä¼˜åŒ–æ–¹æ¡ˆï¼š
1. **å‹ç¼©åä¼ è¾“**: å¯å‡å°‘50-70%å¤§å°
2. **ä½¿ç”¨OSS**: å›½å†…æœåŠ¡å™¨ä¸Šä¼ /ä¸‹è½½éƒ½å¿«
3. **åˆ†å—ä¼ è¾“**: åˆ†å¤šä¸ªå°æ–‡ä»¶ä¼ è¾“
4. **ç›´æ¥åœ¨æœåŠ¡å™¨å‡†å¤‡**: è·³è¿‡ä¼ è¾“æ­¥éª¤

### Q3: Git ä»“åº“å¤ªå¤§ï¼Ÿ

A: ä½¿ç”¨ `.gitignore` æ’é™¤å¤§æ–‡ä»¶ï¼š
```bash
# åªæäº¤ä»£ç å’Œå°æ•°æ®æ–‡ä»¶
data/train_vectorized/
*.tar.gz
*.zip
```

### Q4: æœåŠ¡å™¨æ²¡æœ‰å¤–ç½‘æ€ä¹ˆåŠï¼Ÿ

A: åœ¨æœ¬åœ°ä¸‹è½½å¥½æ‰€æœ‰æ•°æ®å’Œæ¨¡å‹ï¼š
```bash
# æœ¬åœ°ä¸‹è½½æ¨¡å‹
huggingface-cli download Qwen/Qwen2.5-3B-Instruct

# ä¸€èµ·æ‰“åŒ…ä¸Šä¼ 
tar -czf medical_all.tar.gz MedicalGPT/ models/
```

---

## ğŸ“ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬åœ°å‡†å¤‡åï¼š

1. âœ… éªŒè¯æ•°æ®: `python scripts/verify_data.py`
2. âœ… æäº¤åˆ°Git: `git push`
3. âœ… ä¼ è¾“å¤§æ–‡ä»¶åˆ°æœåŠ¡å™¨
4. âœ… åœ¨æœåŠ¡å™¨æ‰§è¡Œè®­ç»ƒ: `bash scripts/run_sft_eval_driven.sh`

---

**æ›´æ–°æ—¶é—´**: 2024å¹´12æœˆ  
**é€‚ç”¨ç³»ç»Ÿ**: Windowsæœ¬åœ° + LinuxæœåŠ¡å™¨
