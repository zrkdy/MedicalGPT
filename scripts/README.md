# Scripts ç›®å½•è¯´æ˜

æœ¬ç›®å½•åŒ…å«æ‰€æœ‰è®­ç»ƒå’Œæµ‹è¯•è„šæœ¬ã€‚

## ğŸ“ æ–‡ä»¶åˆ—è¡¨

### è®­ç»ƒè„šæœ¬

| è„šæœ¬ | ç”¨é€” | è¿è¡Œæ—¶é—´ | GPUè¦æ±‚ |
|------|------|---------|---------|
| `run_pt_qwen2.5-3b.sh` | é¢„è®­ç»ƒï¼ˆå¯é€‰ï¼‰ | 12-18å°æ—¶ | 2Ã—A100 |
| `run_sft_qwen2.5-3b.sh` | ç›‘ç£å¾®è°ƒï¼ˆå¿…éœ€ï¼‰ | 4-8å°æ—¶ | 2Ã—A100 |
| `run_rm_qwen2.5-3b.sh` | å¥–åŠ±å»ºæ¨¡ï¼ˆPPOç”¨ï¼‰ | 2-4å°æ—¶ | 1Ã—A100 |
| `run_dpo_qwen2.5-3b.sh` | DPOè®­ç»ƒï¼ˆæ¨èï¼‰ | 4-6å°æ—¶ | 2Ã—A100 |
| `run_ppo_qwen2.5-3b.sh` | PPOè®­ç»ƒï¼ˆé«˜çº§ï¼‰ | 8-12å°æ—¶ | 2Ã—A100 80GB |

### å·¥å…·è„šæœ¬

| è„šæœ¬ | ç”¨é€” |
|------|------|
| `check_environment.py` | ç¯å¢ƒæ£€æŸ¥å·¥å…· |
| `test_model.py` | æ¨¡å‹æµ‹è¯•å·¥å…· |
| `merge_lora.py` | LoRAæƒé‡åˆå¹¶å·¥å…· |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒæ£€æŸ¥

```bash
# é¦–æ¬¡è¿è¡Œå‰æ£€æŸ¥ç¯å¢ƒ
python scripts/check_environment.py
```

### 2. å¼€å§‹è®­ç»ƒ

```bash
# æœ€ç®€å•çš„æµç¨‹ï¼šSFTè®­ç»ƒ
chmod +x scripts/run_sft_qwen2.5-3b.sh
nohup bash scripts/run_sft_qwen2.5-3b.sh > logs/sft.log 2>&1 &

# ç›‘æ§è®­ç»ƒ
tail -f logs/sft.log
```

### 3. æµ‹è¯•æ¨¡å‹

```bash
# æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹
python scripts/test_model.py
```

### 4. åˆå¹¶LoRAï¼ˆéƒ¨ç½²ç”¨ï¼‰

```bash
python scripts/merge_lora.py \
    --base_model Qwen/Qwen2.5-3B-Instruct \
    --lora_model outputs-sft-qwen2.5-3b/checkpoint-best \
    --output_dir medical-gpt-merged
```

---

## ğŸ“ è®­ç»ƒè„šæœ¬è¯´æ˜

### run_sft_qwen2.5-3b.sh

**åŠŸèƒ½ï¼š** ç›‘ç£å¾®è°ƒï¼Œè®­ç»ƒåŒ»ç–—é—®ç­”èƒ½åŠ›

**å…³é”®å‚æ•°ï¼š**
```bash
--model_name_or_path Qwen/Qwen2.5-3B-Instruct  # åŸºç¡€æ¨¡å‹
--train_file_dir ./data/finetune               # è®­ç»ƒæ•°æ®
--per_device_train_batch_size 2                # æ‰¹æ¬¡å¤§å°
--num_train_epochs 3                           # è®­ç»ƒè½®æ•°
--learning_rate 2e-5                           # å­¦ä¹ ç‡
--lora_rank 16                                 # LoRAç§©
```

**è°ƒæ•´å»ºè®®ï¼š**
- æ˜¾å­˜ä¸è¶³ï¼Ÿå‡å° `batch_size` å’Œ `lora_rank`
- æ•°æ®é‡å¤§ï¼Ÿå¢åŠ  `num_train_epochs`
- æƒ³è¦æ›´å¥½æ•ˆæœï¼Ÿå¢å¤§ `lora_rank` (16â†’32)

### run_dpo_qwen2.5-3b.sh

**åŠŸèƒ½ï¼š** ç›´æ¥åå¥½ä¼˜åŒ–ï¼Œæå‡å›ç­”è´¨é‡

**å…³é”®å‚æ•°ï¼š**
```bash
--model_name_or_path outputs-sft-qwen2.5-3b/checkpoint-best  # åŸºäºSFTæ¨¡å‹
--train_file_dir ./data/reward                               # åå¥½æ•°æ®
--learning_rate 5e-6                                         # è¾ƒå°å­¦ä¹ ç‡
--max_steps 2000                                             # è®­ç»ƒæ­¥æ•°
```

**æ³¨æ„äº‹é¡¹ï¼š**
- å¿…é¡»å…ˆå®ŒæˆSFTè®­ç»ƒ
- DPOæ¯”PPOæ›´ç®€å•ï¼Œæ¨èä½¿ç”¨
- å­¦ä¹ ç‡è¦æ¯”SFTå°

---

## ğŸ› ï¸ å·¥å…·è„šæœ¬è¯´æ˜

### check_environment.py

**åŠŸèƒ½ï¼š** æ£€æŸ¥è®­ç»ƒç¯å¢ƒæ˜¯å¦é…ç½®æ­£ç¡®

**æ£€æŸ¥é¡¹ç›®ï¼š**
- Pythonç‰ˆæœ¬
- CUDAå’ŒGPU
- å¿…éœ€çš„PythonåŒ…
- æ•°æ®æ–‡ä»¶
- ç£ç›˜ç©ºé—´
- HuggingFaceè®¿é—®

**ä½¿ç”¨æ–¹æ³•ï¼š**
```bash
python scripts/check_environment.py
```

### test_model.py

**åŠŸèƒ½ï¼š** æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹

**æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š**
1. æ‰¹é‡æµ‹è¯•ï¼šä½¿ç”¨é¢„è®¾é—®é¢˜
2. äº¤äº’æ¨¡å¼ï¼šå®æ—¶å¯¹è¯

**ä¿®æ”¹æµ‹è¯•æ¨¡å‹ï¼š**
ç¼–è¾‘ `test_model.py` ä¸­çš„é…ç½®ï¼š
```python
lora_model = "outputs-sft-qwen2.5-3b/checkpoint-best"  # æ”¹ä¸ºä½ çš„æ¨¡å‹è·¯å¾„
```

### merge_lora.py

**åŠŸèƒ½ï¼š** åˆå¹¶LoRAæƒé‡ï¼Œä¾¿äºéƒ¨ç½²

**ä½¿ç”¨åœºæ™¯ï¼š**
- éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
- ä½¿ç”¨vLLMç­‰æ¨ç†æ¡†æ¶
- åˆ†äº«å®Œæ•´æ¨¡å‹

**æ³¨æ„ï¼š** åˆå¹¶åæ¨¡å‹ä½“ç§¯ä¼šå¢å¤§ï¼ˆçº¦6GBï¼‰

---

## ğŸ“Š è®­ç»ƒç›‘æ§

### æ–¹æ³•1: æŸ¥çœ‹æ—¥å¿—
```bash
tail -f logs/sft.log
```

### æ–¹æ³•2: TensorBoard
```bash
tensorboard --logdir outputs-sft-qwen2.5-3b --port 6006
# è¿œç¨‹æœåŠ¡å™¨éœ€è¦ç«¯å£è½¬å‘:
# ssh -L 6006:localhost:6006 user@server
```

### æ–¹æ³•3: GPUç›‘æ§
```bash
watch -n 1 nvidia-smi
```

---

## âš™ï¸ å¸¸è§è°ƒæ•´

### æ˜¾å­˜ä¸è¶³ (OOM)
```bash
# åœ¨è®­ç»ƒè„šæœ¬ä¸­ä¿®æ”¹ï¼š
--per_device_train_batch_size 1     # å‡å°æ‰¹æ¬¡
--gradient_accumulation_steps 32    # å¢å¤§ç´¯ç§¯
--lora_rank 8                       # å‡å°LoRAç§©
```

### è®­ç»ƒå¤ªæ…¢
```bash
# å¢å¤§æ‰¹æ¬¡ï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰
--per_device_train_batch_size 4

# å‡å°‘è¯„ä¼°é¢‘ç‡
--eval_steps 1000

# å¯ç”¨FlashAttention
--flash_attn True
```

### æ•ˆæœä¸å¥½
```bash
# å¢å¤§LoRAç§©
--lora_rank 32

# å¢åŠ è®­ç»ƒè½®æ•°
--num_train_epochs 5

# è°ƒæ•´å­¦ä¹ ç‡
--learning_rate 1e-5
```

---

## ğŸ“ è·å–å¸®åŠ©

é‡åˆ°é—®é¢˜ï¼Ÿ
1. æ£€æŸ¥ `TRAINING_GUIDE_Qwen2.5-3B.md` è¯¦ç»†æŒ‡å—
2. è¿è¡Œ `python scripts/check_environment.py` è¯Šæ–­ç¯å¢ƒ
3. æŸ¥çœ‹é¡¹ç›® Issues: https://github.com/shibing624/MedicalGPT/issues
